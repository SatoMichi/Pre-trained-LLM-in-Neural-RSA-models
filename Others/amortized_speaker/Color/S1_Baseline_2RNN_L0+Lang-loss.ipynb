{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from corpus import ColorsCorpusReader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_data(cols, contexts, id=0):\n",
    "    col01,col02,col03 = cols[id]\n",
    "    context = contexts[id]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 3))\n",
    "    fig.suptitle(context)\n",
    "    # plot correct color, col01\n",
    "    ec = col01\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col01, ec=ec, lw=8)\n",
    "    axes[0].add_patch(patch)\n",
    "    axes[0].axis('off')\n",
    "    #axes[0].set_title(str(col01))\n",
    "    # plot wrong color, col02\n",
    "    ec = col02\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col02, ec=ec, lw=8)\n",
    "    axes[1].add_patch(patch)\n",
    "    axes[1].axis('off')\n",
    "    #axes[1].set_title(str(col02))\n",
    "    # plot wrong color, col02\n",
    "    ec = \"black\"\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col03, ec=ec, lw=8)\n",
    "    axes[2].add_patch(patch)\n",
    "    axes[2].axis('off')\n",
    "    #axes[2].set_title(str(col03))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2index(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [vocab_dict[w] for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare raw data\n",
    "root = Path(os.path.abspath('')).parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\")\n",
    "print(data_path)\n",
    "corpus = ColorsCorpusReader(os.path.join(data_path,\"colors.csv\"), word_count=None, normalize_colors=True)\n",
    "examples = list(corpus.read())\n",
    "print(\"Number of datapoints: {}\".format(len(examples)))\n",
    "# balance positive and negative samples\n",
    "colors_data = [e.get_context_data()[0] for e in examples]\n",
    "utterance_data = [e.get_context_data()[1] for e in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_row_data(colors_data,utterance_data,id=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pickle\n",
    "# generate vocab dict\n",
    "if not os.path.exists(\"vocab.pkl\"):\n",
    "    print(\"Generating vocab dict ...\")\n",
    "    vocab_list = list(set(reduce(lambda x,y:x+y,[word_tokenize(c) for c in utterance_data]))) # with nltk.tokenizer, 3953 vocabs\n",
    "    vocab_list = [\"<pad>\",\"<sos>\",\"<eos>\",\"<unk>\"] + vocab_list                               # Added padding for batching\n",
    "    vocab_dict = dict(zip(vocab_list,list(range(len(vocab_list)))))\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab_dict, f)\n",
    "else:\n",
    "    print(\"Loading vocab dict ...\")\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab_dict = pickle.load(f)\n",
    "print(\"Length of the Vocab list is \",len(vocab_dict.keys()))\n",
    "print(\"<pad> id = \",vocab_dict[\"<pad>\"])\n",
    "print(\"<sos> id = \",vocab_dict[\"<sos>\"])\n",
    "print(\"<eos> id = \",vocab_dict[\"<eos>\"])\n",
    "print(\"<unk> id = \",vocab_dict[\"<unk>\"])\n",
    "print(\"blue id = \",vocab_dict[\"blue\"])\n",
    "print(\"red id = \",vocab_dict[\"red\"])\n",
    "print(\"green id = \",vocab_dict[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = vocab_dict\n",
    "i2w = {k:v for (v,k) in vocab_dict.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "colors_data_tensor = torch.tensor(np.array(colors_data),dtype=torch.float)\n",
    "context_id_data = list(map(sentence2index,utterance_data))\n",
    "max_context_len = max([len(c) for c in context_id_data])\n",
    "padded_context_data = torch.tensor(np.array([[1]+c+[2]+[0]*(max_context_len-len(c)) for c in context_id_data]))   # <sos>+context+<eos>+<pad>*\n",
    "print(\"Colors shape = \",colors_data_tensor.shape)\n",
    "print(\"Padded context id lists shape = \",padded_context_data.shape)\n",
    "\n",
    "data = [(color,torch.tensor(context,dtype=torch.long)) for color,context in zip(colors_data_tensor,padded_context_data)]\n",
    "label = torch.zeros(len(data),3)\n",
    "label[:,2] = 1.0\n",
    "print(\"total data length = \",len(data))\n",
    "print(\"total label shape = \",label.shape)\n",
    "\n",
    "test_split = 7000\n",
    "train_data, test_data = data[:-test_split], data[-test_split:]\n",
    "train_label, test_label = label[:-test_split], label[-test_split:]\n",
    "print(\"Train, Test data length = \",len(train_data),\",\",len(test_data))\n",
    "print(\"Train, Test label length = \",len(train_label),\",\",len(test_label))\n",
    "\n",
    "train_dataset = list(zip(train_data,train_label))\n",
    "test_dataset = list(zip(test_data,test_label))\n",
    "train_batch = DataLoader(dataset=train_dataset,batch_size=128,shuffle=True,num_workers=0)\n",
    "test_batch = DataLoader(dataset=test_dataset,batch_size=128,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (cols, lang), label in train_batch:\n",
    "    print(cols.shape)\n",
    "    print(lang.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(y, n):\n",
    "    y_onehot = torch.zeros(y.shape[0], n).to(y.device)\n",
    "    y_onehot.scatter_(1, y.view(-1, 1), 1)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For permutation invariance\n",
    "class Colors_DeepSet(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=16):\n",
    "        super(Colors_DeepSet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, output_size)\n",
    "        self.linear2 = nn.Linear(output_size, output_size)\n",
    "    \n",
    "    def forward(self, col1, col2):\n",
    "        col_emb1 = F.relu(self.linear1(col1))\n",
    "        col_emb2 = F.relu(self.linear1(col2))\n",
    "        col_embs = col_emb1 + col_emb2\n",
    "        col_embs = self.linear2(col_embs)\n",
    "        return col_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors_Feature(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size =16):\n",
    "        super(Colors_Feature, self).__init__()\n",
    "        self.deepset = Colors_DeepSet(input_size, output_size)\n",
    "        self.linear = nn.Linear(input_size+output_size, output_size)\n",
    "\n",
    "    def forward(self,feats,labels):\n",
    "        idxs = [0,1,2]\n",
    "        target_idx = int(torch.argmax(labels))\n",
    "        idxs.remove(target_idx)\n",
    "        other_idx1,other_idx2 = idxs[0],idxs[1]\n",
    "        target_col,col1,col2 = feats[:,target_idx], feats[:,other_idx1], feats[:,other_idx2]\n",
    "        col_embs = F.relu(self.deepset(col1,col2))\n",
    "        cols = torch.hstack((target_col,col_embs))\n",
    "        feat = self.linear(cols)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "    def __init__(self, feat_model, embedding_module, feat_size=16, hidden_size=100):\n",
    "        super(Speaker, self).__init__()\n",
    "        self.embedding = embedding_module\n",
    "        self.feat_model = feat_model\n",
    "        self.embedding_dim = embedding_module.embedding_dim\n",
    "        self.vocab_size = embedding_module.num_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, num_layers=2, bidirectional=True)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size*2, self.vocab_size)                             # *2 for bidirectioanl\n",
    "        self.init_h = nn.Linear(feat_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, feats, labels, lang, tau=1, max_len=10):\n",
    "        feats_emb = self.feat_model(feats, labels)\n",
    "        states = self.init_h(feats_emb).unsqueeze(0)\n",
    "        states = states.repeat(2*2,1,1)\n",
    "        #print(lang.shape)\n",
    "        # Trainig based on langauge model next word prediction\n",
    "        embedded = self.embedding(lang)\n",
    "        embedded = embedded.transpose(0, 1)                               # (B,L,D) to (L,B,D)\n",
    "        #print(embedded.shape)\n",
    "        outputs,states = self.gru(embedded, states)\n",
    "        outputs = self.outputs2vocab(outputs)\n",
    "        predicted_onehot = F.gumbel_softmax(outputs, tau=tau, hard=True)\n",
    "        train_lang_tensor = predicted_onehot.transpose(0, 1)\n",
    "        # Trainig based on generation model based on L0 loss etc.\n",
    "        gen_lang_tensor = self.generate(feats,states,tau=tau,max_len=max_len)\n",
    "        return train_lang_tensor, gen_lang_tensor\n",
    "    \n",
    "    def generate(self,feats,states,tau=1, max_len=10):\n",
    "        batch_size = feats.size(0)\n",
    "        # This contains are series of sampled onehot vectors\n",
    "        lang = []\n",
    "        # first input is SOS token\n",
    "        inputs_onehot = torch.zeros(batch_size, self.vocab_size).to(feats.device)   # (batch_size, n_vocab)\n",
    "        inputs_onehot[:, SOS] = 1.0\n",
    "        inputs_onehot = inputs_onehot.unsqueeze(1)                                  # (batch_size, len, n_vocab)\n",
    "        lang.append(inputs_onehot)\n",
    "        # initialize first inputs\n",
    "        inputs_onehot = inputs_onehot.transpose(0, 1)                               # (B,L,D) to (L,B,D)\n",
    "        inputs = inputs_onehot @ self.embedding.weight                              # (batch_size, 1, n_vocab) X (n_vocab, h) -> (batch_size, 1, h)\n",
    "        for i in range(max_len - 2):  # Have room for SOS, EOS if never sampled\n",
    "            self.gru.flatten_parameters()\n",
    "            outputs, states = self.gru(inputs, states)  # outputs: (L=1,B,H)\n",
    "            outputs = outputs.squeeze()                 # outputs: (B,H)\n",
    "            outputs = self.outputs2vocab(outputs)       # outputs: (B,V)\n",
    "            predicted_onehot = F.gumbel_softmax(outputs, tau=tau, hard=True)    # (B,V)\n",
    "            lang.append(predicted_onehot.unsqueeze(1))\n",
    "            inputs = (predicted_onehot.unsqueeze(0)) @ self.embedding.weight    # (1, batch_size, n_vocab) X (n_vocab, h) -> (1, batch_size, h)\n",
    "        # Add EOS if we've never sampled it\n",
    "        eos_onehot = torch.zeros(batch_size, 1, self.vocab_size).to(feats.device)\n",
    "        eos_onehot[:, 0, EOS] = 1.0\n",
    "        lang.append(eos_onehot)\n",
    "        # Cat language tensors\n",
    "        lang_tensor = torch.cat(lang, 1)                    # (B,max_L,V)\n",
    "        # Sum up log probabilities of samples\n",
    "        return lang_tensor\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipeline check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def eval_perplexity(model, batch, criterion, vocab_size):\n",
    "  loss_sum = 0\n",
    "  hidden1, hidden2 = None, None\n",
    "  # 勾配を計算しないモードへ\n",
    "  with torch.no_grad():\n",
    "    # モデルを評価モードへ\n",
    "    model.eval()\n",
    "    for i,((cols,lang),label) in enumerate(batch):\n",
    "      cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "      lang_target = lang[:,1:]\n",
    "      lang_tensor,_ = model(cols, label, lang[:,:-1], tau=5)\n",
    "      loss = criterion(lang_tensor.reshape(-1, vocab_size), lang_target.reshape(-1))\n",
    "      loss_sum += loss.item()\n",
    "    ppl = math.exp(loss_sum / len(batch))\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_color import SimpleBaseLine_L0\n",
    "\n",
    "emb_dim = 768\n",
    "speaker_embs = nn.Embedding(len(vocab_dict), emb_dim)\n",
    "speaker_feat = Colors_Feature(output_size=16)\n",
    "speaker = Speaker(speaker_feat, speaker_embs)\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = SimpleBaseLine_L0(len(vocab_dict)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params/baseline_fixed-vocab_l0.pth\",map_location=device))\n",
    "\n",
    "max_len = 5\n",
    "\n",
    "optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "pps = []\n",
    "for i,((cols,lang),label) in enumerate(train_batch):\n",
    "    cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    literal_listener.eval()\n",
    "    speaker.train()\n",
    "    lang_tensor, gen_lang_tensor = speaker(cols, label, lang[:,:-1], tau=5)\n",
    "    # for lang loss \n",
    "    lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(vocab_dict.keys())).unsqueeze(0) for sent in lang]))\n",
    "    lang_target = lang_onehot[:,1:,:]\n",
    "    lang_loss = criterion(lang_tensor.reshape(-1,len(vocab_dict)),lang_target.reshape(-1,len(vocab_dict)))\n",
    "    # for L0 loss\n",
    "    output_lang = gen_lang_tensor.argmax(2)\n",
    "    lis_scores01 = literal_listener(cols[:,0], output_lang)\n",
    "    lis_scores02 = literal_listener(cols[:,1], output_lang)\n",
    "    lis_scores03 = literal_listener(cols[:,2], output_lang)\n",
    "    lis_labels = torch.hstack((lis_scores01,lis_scores02,lis_scores03))\n",
    "    lis_loss = criterion(lis_labels,label)\n",
    "    # total loss\n",
    "    loss = lang_loss + lis_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%10 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        print(\"\\nOriginal sentence:\\n\"+\" \".join([i2w[idx] for idx in lang[id].to(\"cpu\").tolist()]).replace(\" <pad>\",\"\"))\n",
    "        print(\"Output sentence:\\n\"+\" \".join([i2w[idx] for idx in lang_tensor.argmax(2)[id].to(\"cpu\").tolist()]).replace(\" <pad>\",\"\")+\"\\n\")\n",
    "        print(\"Generated sentence:\\n\"+\" \".join([i2w[idx] for idx in gen_lang_tensor.argmax(2)[id].to(\"cpu\").tolist()]).replace(\" <pad>\",\"\")+\"\\n\")\n",
    "        print(\"Loss: \",loss.item())\n",
    "        pp = eval_perplexity(speaker,test_batch,criterion,len(vocab_dict))\n",
    "        print(\"Perplexity:\",pp)\n",
    "        pps.append(pp)\n",
    "    if i > 10: break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_color import SimpleBaseLine_L0\n",
    "\n",
    "emb_dim = 768\n",
    "speaker_embs = nn.Embedding(len(vocab_dict), emb_dim)\n",
    "speaker_feat = Colors_Feature(output_size=16)\n",
    "speaker = Speaker(speaker_feat, speaker_embs)\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = SimpleBaseLine_L0(len(vocab_dict)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params/baseline_fixed-vocab_l0.pth\",map_location=device))\n",
    "\n",
    "#max_len = max_context_len+2  # <sos> + max_len + <eos>\n",
    "max_len = 5\n",
    "\n",
    "optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(cols, label, c_lang, g_lang, lis_label):\n",
    "    col01,col02,col03 = cols[0].numpy(), cols[1].numpy(), cols[2].numpy()\n",
    "    context = \"Correct: \"+\" \".join(c_lang)+\"\\nGenerated:\"+\" \".join(g_lang)+\"\\n \"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 6))\n",
    "    fig.suptitle(context)\n",
    "    # plot correct color, col01\n",
    "    ec = \"black\" if label[0] > 0.5 else col01\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col01, ec=ec, lw=8)\n",
    "    axes[0].add_patch(patch)\n",
    "    axes[0].axis('off')\n",
    "    if torch.argmax(lis_label)==0:axes[0].set_title(\"Listener Prediction\")\n",
    "    # plot wrong color, col02\n",
    "    ec = \"black\" if label[1] > 0.5 else col02\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col02, ec=ec, lw=8)\n",
    "    axes[1].add_patch(patch)\n",
    "    axes[1].axis('off')\n",
    "    if torch.argmax(lis_label)==1:axes[1].set_title(\"Listener Prediction\")\n",
    "    # plot wrong color, col02\n",
    "    ec = \"black\" if label[2] > 0.5 else col03\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col03, ec=ec, lw=8)\n",
    "    axes[2].add_patch(patch)\n",
    "    axes[2].axis('off')\n",
    "    if torch.argmax(lis_label)==2:axes[2].set_title(\"Listener Prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "train_pp_list = []\n",
    "test_pp_list = []\n",
    "best_loss = 100\n",
    "best_pp = 10000\n",
    "for i in range(epoch):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_pp = 0\n",
    "    test_pp = 0\n",
    "\n",
    "    literal_listener.eval()\n",
    "    #speaker.train()\n",
    "    for j, ((cols,lang),label) in enumerate(train_batch):\n",
    "        cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "        speaker.train()\n",
    "        lang_tensor, gen_lang_tensor = speaker(cols, label, lang[:,:-1], tau=5, max_len=max_len)\n",
    "        # for lang loss \n",
    "        lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(vocab_dict.keys())).unsqueeze(0) for sent in lang]))\n",
    "        lang_target = lang_onehot[:,1:,:]\n",
    "        lang_loss = criterion(lang_tensor.reshape(-1,len(vocab_dict)),lang_target.reshape(-1,len(vocab_dict)))\n",
    "        # for L0 loss\n",
    "        output_lang = gen_lang_tensor.argmax(2)\n",
    "        lis_scores01 = literal_listener(cols[:,0], output_lang)\n",
    "        lis_scores02 = literal_listener(cols[:,1], output_lang)\n",
    "        lis_scores03 = literal_listener(cols[:,2], output_lang)\n",
    "        lis_labels = F.softmax(torch.hstack((lis_scores01,lis_scores02,lis_scores03)))\n",
    "        lis_loss = criterion(lis_labels,label)\n",
    "        # total loss\n",
    "        loss = lang_loss + lis_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_pp += eval_perplexity(speaker,train_batch,criterion,len(vocab_dict))\n",
    "        if j%100==0: print(j+1,\"/\",len(train_batch))\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    batch_train_pp = train_pp/len(train_batch)\n",
    "\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for (cols,lang),label in test_batch:\n",
    "            cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "            lang_tensor, gen_lang_tensor = speaker(cols, label, lang[:,:-1], tau=5, max_len=max_len)\n",
    "            # for lang loss \n",
    "            lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(vocab_dict.keys())).unsqueeze(0) for sent in lang]))\n",
    "            lang_target = lang_onehot[:,1:,:]\n",
    "            lang_loss = criterion(lang_tensor.reshape(-1,len(vocab_dict)),lang_target.reshape(-1,len(vocab_dict)))\n",
    "            # for L0 loss\n",
    "            output_lang = gen_lang_tensor.argmax(2)\n",
    "            lis_scores01 = literal_listener(cols[:,0], output_lang)\n",
    "            lis_scores02 = literal_listener(cols[:,1], output_lang)\n",
    "            lis_scores03 = literal_listener(cols[:,2], output_lang)\n",
    "            lis_labels = F.softmax(torch.hstack((lis_scores01,lis_scores02,lis_scores03)))\n",
    "            lis_loss = criterion(lis_labels,label)\n",
    "            # total loss\n",
    "            loss = lang_loss + lis_loss\n",
    "            test_loss += loss.item()\n",
    "            test_pp += eval_perplexity(speaker,test_batch,criterion,len(vocab_dict))\n",
    "        batch_test_loss = test_loss/len(test_batch)\n",
    "        batch_test_pp = test_pp/len(test_batch)\n",
    "\n",
    "    print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "    print(\"Train pp:{:.2E}, Test pp:{:.2E}\".format(batch_train_pp,batch_test_pp))\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    test_loss_list.append(batch_test_loss)\n",
    "    train_pp_list.append(batch_train_pp)\n",
    "    test_pp_list.append(batch_test_pp)\n",
    "    if batch_test_loss < best_loss:\n",
    "        print(\"Best loss saved ...\")\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/color_S1_lis=baseline_2-birnnX2_no-penalty_loss=L0+Lang_best_loss.pth\")\n",
    "        best_loss = batch_test_loss\n",
    "    if batch_test_pp < best_pp:\n",
    "        print(\"Best pp saved ...\")\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/color_S1_lis=baseline_2-birnnX2_no-penalty_loss=L0+Lang_best_pp.pth\")\n",
    "        best_pp = batch_test_pp\n",
    "    if i%1 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        cols = cols[id].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]\n",
    "        g_langs = [i2w[idx] for idx in lang_tensor.argmax(2)[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]\n",
    "        check_data(cols, label[id], c_langs, g_langs, label[id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure()\n",
    "plt.title(\"Train and Test Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(range(1,epoch+1),train_loss_list,\"b-\",label=\"train_loss\")\n",
    "plt.plot(range(1,epoch+1),test_loss_list,\"r--\",label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "train_acc_list = [float(acc) for acc in train_pp_list]\n",
    "test_acc_list = [float(acc) for acc in test_pp_list]\n",
    "plt.figure()\n",
    "plt.title(\"Train and Test Perplexity\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.plot(range(1,epoch+1),train_acc_list,\"b-\",label=\"train_pp\")\n",
    "plt.plot(range(1,epoch+1),test_acc_list,\"r--\",label=\"test_pp\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker.load_state_dict(torch.load(\"model_params/color_S1_lis=baseline_2-birnnX2_no-penalty_loss=L0+Lang_best_loss.pth\",map_location=device))\n",
    "speaker.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 communication Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss = []\n",
    "accs = []\n",
    "speaker.eval()\n",
    "\n",
    "for i,((cols,lang),label) in enumerate(test_batch):\n",
    "    cols, lang = cols.to(device), lang.to(device)\n",
    "    label = label.to(device)\n",
    "    literal_listener.eval()\n",
    "    lang_tensor,gen_lang_tensor= speaker(cols, label, lang[:,:-1], tau=5, max_len=max_len)\n",
    "    output_lang = gen_lang_tensor.argmax(2)\n",
    "    lis_scores01 = literal_listener(cols[:,0], output_lang)\n",
    "    lis_scores02 = literal_listener(cols[:,1], output_lang)\n",
    "    lis_scores03 = literal_listener(cols[:,2], output_lang)\n",
    "    lis_labels = F.softmax(torch.hstack((lis_scores01,lis_scores02,lis_scores03)))\n",
    "    loss = criterion(lis_labels,label)\n",
    "    pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])+2\n",
    "    acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "    #print(\"Accuracy:\",acc)\n",
    "    accs.append(acc.item())\n",
    "    if i%10 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        cols = cols[id].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]\n",
    "        g_langs = [i2w[idx] for idx in gen_lang_tensor.argmax(2)[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]\n",
    "        label = label[id]\n",
    "        print(int(torch.argmax(lis_labels[id])))\n",
    "        check_data(cols, label, c_langs, g_langs, lis_labels[id])\n",
    "        \n",
    "    #if i > 100: break\n",
    "\n",
    "print(\"Accuracy:,\",np.mean(accs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss1 = []\n",
    "accs1 = []\n",
    "losss2 = []\n",
    "accs2 = []\n",
    "speaker.eval()\n",
    "\n",
    "for i,((cols,lang),label) in enumerate(test_batch):\n",
    "    cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "    # for 1st image\n",
    "    label01 = torch.zeros_like(label)\n",
    "    label01[:0] = 1.0\n",
    "    lang_tensor1,gen_lang_tensor1 = speaker(cols, label01, lang[:,:-1], tau=5, max_len=max_len)\n",
    "    output_lang1 = gen_lang_tensor1.argmax(2)\n",
    "    # for 2nd image\n",
    "    label02 = torch.zeros_like(label)\n",
    "    label02[:,1] = 1.0\n",
    "    lang_tensor2,gen_lang_tensor2 = speaker(cols, label02, lang[:,:-1], tau=5, max_len=max_len)\n",
    "    output_lang2 = gen_lang_tensor2.argmax(2)\n",
    "    # for 3rd image\n",
    "    label03 = torch.zeros_like(label)\n",
    "    label03[:,2] = 1.0\n",
    "    lang_tensor3,gen_lang_tensor3 = speaker(cols, label03, lang[:,:-1], tau=5, max_len=max_len)\n",
    "    output_lang3 = gen_lang_tensor3.argmax(2)\n",
    "    \n",
    "    prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_tensor1,lang))]\n",
    "    prob01_sums = list(map(sum,prob01))\n",
    "    prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_tensor2,lang))]\n",
    "    prob02_sums = list(map(sum,prob02))\n",
    "    prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_tensor3,lang))]\n",
    "    prob03_sums = list(map(sum,prob03))\n",
    "    lang_probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "\n",
    "    # from generated language\n",
    "    prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(gen_lang_tensor1,lang))]\n",
    "    prob01_sums = list(map(sum,prob01))\n",
    "    prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(gen_lang_tensor2,lang))]\n",
    "    prob02_sums = list(map(sum,prob02))\n",
    "    prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(gen_lang_tensor3,lang))]\n",
    "    prob03_sums = list(map(sum,prob03))\n",
    "    gen_probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "    #print(probs)\n",
    "    # save loss\n",
    "    loss = criterion(lang_probs.to(device),label)\n",
    "    losss1.append(loss.item())\n",
    "    loss = criterion(gen_probs.to(device),label) \n",
    "    losss2.append(loss.item())\n",
    "    # save acc\n",
    "    pred_labels = torch.argmax(lang_probs,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])+2\n",
    "    acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "    accs1.append(acc.item())\n",
    "    pred_labels = torch.argmax(gen_probs,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])+2\n",
    "    acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "    accs2.append(acc.item())\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        imgs = cols[id].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[id].to(\"cpu\").tolist() if idx not in [EOS,SOS,PAD]]\n",
    "        g_langs = [i2w[idx] for idx in output_lang1[id].to(\"cpu\").tolist() if idx not in [EOS,SOS,PAD]] \\\n",
    "                + [\"|\"]+ [i2w[idx] for idx in output_lang2[id].to(\"cpu\").tolist() if idx not in [EOS,SOS,PAD]] \\\n",
    "                + [\"|\"]+ [i2w[idx] for idx in output_lang3[id].to(\"cpu\").tolist() if idx not in [EOS,SOS,PAD]]\n",
    "        label = label[id]\n",
    "        check_data(imgs, label, c_langs, g_langs, lang_probs[id])\n",
    "        print(torch.exp(lang_probs[id]))\n",
    "        print(torch.where(torch.exp(lang_probs[id])>0.1,1,0))\n",
    "        check_data(imgs, label, c_langs, g_langs, gen_probs[id])\n",
    "        print(torch.exp(gen_probs[id]))\n",
    "        print(torch.where(torch.exp(gen_probs[id])>0.1,1,0))\n",
    "        #print(\"Loss: \",loss.item())\n",
    "\n",
    "print(\"Loss: \",np.mean(losss1))\n",
    "print(\"Accuracy: \",np.mean(accs1))\n",
    "print(\"Loss: \",np.mean(losss2))\n",
    "print(\"Accuracy: \",np.mean(accs2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
