{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from shapeworld_data import load_raw_data, get_vocab, ShapeWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_raw_data(imgs, labels, langs, id=0):\n",
    "    data = list(zip(imgs,labels,langs))\n",
    "    img_list,label,lang = data[id]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 2))\n",
    "    fig.suptitle(\" \".join(lang))\n",
    "    for i,(l,img) in enumerate(zip(label,img_list)):\n",
    "        img = img.transpose((2,1,0))\n",
    "        axes[i].imshow(img)\n",
    "        if l==1: axes[i].set_title(\"Correct\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.path.abspath('')).parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\\shapeworld_np\")\n",
    "print(data_path)\n",
    "data_list = os.listdir(data_path)\n",
    "print(data_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vocab_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab([os.path.join(data_path,d) for d in data_list])\n",
    "print(vocab[\"w2i\"])\n",
    "\n",
    "COLOR = {\"white\":[1,0,0,0,0,0], \"green\":[0,1,0,0,0,0], \"gray\":[0,0,1,0,0,0], \"yellow\":[0,0,0,1,0,0], \"red\":[0,0,0,0,1,0], \"blue\":[0,0,0,0,0,1], \"other\":[0,0,0,0,0,0]}\n",
    "SHAPE = {\"shape\":[0,0,0,0], \"square\":[1,0,0,0], \"circle\":[0,1,0,0], \"rectangle\":[0,0,1,0], \"ellipse\":[0,0,0,1]}\n",
    "\n",
    "w2i = vocab[\"w2i\"]\n",
    "i2w = vocab[\"i2w\"]\n",
    "\n",
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[0]))\n",
    "print(d[\"imgs\"].shape)\n",
    "print(d[\"labels\"].shape)\n",
    "print(d[\"langs\"].shape)\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"])\n",
    "train_batch = DataLoader(ShapeWorld(d, vocab), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[1]))\n",
    "print(d[\"imgs\"].shape)\n",
    "print(d[\"labels\"].shape)\n",
    "print(d[\"langs\"].shape)\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"],id=1)\n",
    "test_batch = DataLoader(ShapeWorld(d, vocab), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, label, lang in train_batch:\n",
    "    print(imgs.shape)\n",
    "    print(lang.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(y, n=3):\n",
    "    #print(y.shape)\n",
    "    y_onehot = torch.zeros(y.shape[0], n).to(y.device)\n",
    "    y_onehot.scatter_(1, y.to(torch.int64).view(-1, 1), 1)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imgs_emb_DeepSet(nn.Module):\n",
    "    def __init__(self, input_size=1024, output_size=1024):\n",
    "        super(Imgs_emb_DeepSet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, img_emb1, img_emb2):\n",
    "        img_embs = img_emb1 + img_emb2\n",
    "        img_embs = self.linear1(img_embs)\n",
    "        return img_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision import ConvNet\n",
    "from literal_listener_shapeworld import CNN_encoder\n",
    "\n",
    "class Imgs_Feature(nn.Module):\n",
    "    def __init__(self, input_size=1024, output_size =1024):\n",
    "        super(Imgs_Feature, self).__init__()\n",
    "        self.cnn_encoder = ConvNet(4)\n",
    "        self.deepset = Imgs_emb_DeepSet(input_size, output_size)\n",
    "        self.linear = nn.Linear(output_size+input_size, output_size)\n",
    "\n",
    "    def forward(self,feats):\n",
    "        batch_size = feats.shape[0]\n",
    "        n_obj = feats.shape[1]\n",
    "        rest = feats.shape[2:]\n",
    "        feats_flat = feats.reshape(batch_size * n_obj, *rest)\n",
    "        feats_emb_flat = self.cnn_encoder(feats_flat)\n",
    "        cnn_emb = feats_emb_flat.unsqueeze(1).view(batch_size, n_obj, -1)\n",
    "        target_img,img1,img2 = cnn_emb[:,0,:], cnn_emb[:,1,:], cnn_emb[:,2,:]\n",
    "        img_embs = F.relu(self.deepset(img1,img2))\n",
    "        imgs = torch.hstack((img_embs,target_img))\n",
    "        #print(cols.shape)\n",
    "        feat = self.linear(imgs)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "    def __init__(self, feat_model, embedding_module, feat_size=1024, hidden_size=100):\n",
    "        super(Speaker, self).__init__()\n",
    "        self.embedding = embedding_module\n",
    "        self.feat_model = feat_model\n",
    "        self.embedding_dim = embedding_module.embedding_dim\n",
    "        self.vocab_size = embedding_module.num_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size, bidirectional=False)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size*1, self.vocab_size)                             # *2 for bidirectioanl\n",
    "        self.init_h = nn.Linear(feat_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, feats, tau=1, length_penalty=False, max_len=40):\n",
    "        batch_size = feats.size(0)\n",
    "\n",
    "        # initialize hidden states using image features\n",
    "        feats_emb = self.feat_model(feats)\n",
    "        states = self.init_h(feats_emb).unsqueeze(0)\n",
    "        #states = torch.vstack((states, states))\n",
    "        \n",
    "        # This contains are series of sampled onehot vectors\n",
    "        lang,eos_prob,lang_prob = [], [], []\n",
    "        lang_length = torch.ones(batch_size, dtype=torch.int64).to(feats.device)\n",
    "        done_sampling = [False for _ in range(batch_size)]\n",
    "\n",
    "        # first input is SOS token\n",
    "        inputs_onehot = torch.zeros(batch_size, self.vocab_size).to(feats.device)   # (batch_size, n_vocab)\n",
    "        inputs_onehot[:, SOS] = 1.0\n",
    "        inputs_onehot = inputs_onehot.unsqueeze(1)                                  # (batch_size, 1, n_vocab)\n",
    "        lang.append(inputs_onehot)                                                  # Add SOS to lang\n",
    "        \n",
    "        inputs_onehot = inputs_onehot.transpose(0, 1)                               # (B,L,D) to (L,B,D)\n",
    "        inputs = inputs_onehot @ self.embedding.weight                              # (1,batch_size, n_vocab) X (n_vocab, h) -> (1,batch_size, h)\n",
    "\n",
    "        for i in range(max_len - 2):  # Have room for SOS, EOS if never sampled\n",
    "            if all(done_sampling): break\n",
    "            \n",
    "            self.gru.flatten_parameters()\n",
    "            #print(inputs.shape)\n",
    "            #print(states.shape)\n",
    "            outputs, states = self.gru(inputs, states)                          # outputs: (L=1,B,H)\n",
    "            outputs = outputs.squeeze()                                         # outputs: (B,H)\n",
    "            outputs = self.outputs2vocab(outputs)                               # outputs: (B,V)\n",
    "            predicted_onehot = F.gumbel_softmax(outputs, tau=tau, hard=True)    # (B,V)\n",
    "            lang.append(predicted_onehot.unsqueeze(1))                          # Add to lang\n",
    "            \n",
    "            if length_penalty:\n",
    "                idx_prob = F.log_softmax(outputs, dim = 1)\n",
    "                eos_prob.append(idx_prob[:,EOS])\n",
    "\n",
    "            predicted_npy = predicted_onehot.argmax(1).cpu().numpy()            # (B,1)\n",
    "            \n",
    "            # Update language lengths\n",
    "            for j, pred in enumerate(predicted_npy):\n",
    "                if not done_sampling[j]: lang_length[j] += 1\n",
    "                if pred == EOS: done_sampling[j] = True\n",
    "\n",
    "            inputs = (predicted_onehot.unsqueeze(0)) @ self.embedding.weight    # (1, batch_size, n_vocab) X (n_vocab, h) -> (1, batch_size, h)\n",
    "        \n",
    "        \n",
    "        self.gru.flatten_parameters()\n",
    "        outputs, states = self.gru(inputs, states)  # outputs: (L=1,B,H)\n",
    "        outputs = outputs.squeeze(0)                # outputs: (B,H)\n",
    "        outputs = self.outputs2vocab(outputs)       # outputs: (B,V)\n",
    "        idx_prob = F.log_softmax(outputs, dim=1)    # (B,V)\n",
    "        lang_prob.append(idx_prob[:, EOS].unsqueeze(1))    # lang.append((B,1))\n",
    "            \n",
    "        # Add EOS if we've never sampled it\n",
    "        eos_onehot = torch.zeros(batch_size, 1, self.vocab_size).to(feats.device)\n",
    "        eos_onehot[:, 0, EOS] = 1.0\n",
    "        lang.append(eos_onehot)\n",
    "        for i, _ in enumerate(predicted_npy):               #predicted_npy: (B,1)\n",
    "            if not done_sampling[i]: lang_length[i] += 1\n",
    "            done_sampling[i] = True\n",
    "\n",
    "        # Cat language tensors\n",
    "        lang_tensor = torch.cat(lang, 1)                    # (B,max_L,V)\n",
    "        # Trim max length\n",
    "        for i in range(lang_tensor.shape[0]):\n",
    "            lang_tensor[i, lang_length[i]:] = 0\n",
    "        max_lang_len = max_len #lang_length.max()\n",
    "        lang_tensor = lang_tensor[:, :max_lang_len, :]\n",
    "        \n",
    "        lang_prob_tensor = torch.cat(lang_prob, 1)              # (B,arbital_L)\n",
    "        for i in range(lang_prob_tensor.shape[0]):\n",
    "            lang_prob_tensor[i, lang_length[i]:] = 0\n",
    "        lang_prob_tensor = lang_prob_tensor[:, :max_lang_len]   # (B,arbital_L)\n",
    "        lang_prob = lang_prob_tensor.sum(1)                     # (B,1)\n",
    "        \n",
    "        if length_penalty:\n",
    "            # eos prob -> eos loss\n",
    "            eos_prob = torch.stack(eos_prob, dim = 1)\n",
    "            for i in range(eos_prob.shape[0]):\n",
    "                r_len = torch.arange(1,eos_prob.shape[1]+1,dtype=torch.float32)\n",
    "                eos_prob[i] = eos_prob[i]*r_len.to(eos_prob.device)\n",
    "                eos_prob[i, lang_length[i]:] = 0\n",
    "            eos_loss = -eos_prob\n",
    "            eos_loss = eos_loss.sum(1)/lang_length.float()\n",
    "            eos_loss = eos_loss.mean()\n",
    "        else:\n",
    "            eos_loss = 0\n",
    "            \n",
    "        # Sum up log probabilities of samples\n",
    "        return lang_tensor, lang_length, eos_loss, lang_prob\n",
    "\n",
    "    def to_text(self, lang_onehot):\n",
    "        texts = []\n",
    "        lang = lang_onehot.argmax(2)\n",
    "        for sample in lang.cpu().numpy():\n",
    "            text = []\n",
    "            for item in sample:\n",
    "                text.append(data.ITOS[item])\n",
    "                if item == data.EOS_IDX:\n",
    "                    break\n",
    "            texts.append(' '.join(text))\n",
    "        return np.array(texts, dtype=np.unicode_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "    def __init__(self, feat_model, embedding_module, hidden_size=100):\n",
    "        super(Speaker, self).__init__()\n",
    "        self.embedding = embedding_module\n",
    "        self.feat_model = feat_model\n",
    "        self.feat_size = feat_model.final_feat_dim\n",
    "        self.embedding_dim = embedding_module.embedding_dim\n",
    "        self.vocab_size = embedding_module.num_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.hidden_size)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        # n_obj of feature size + 1/0 indicating target index\n",
    "        self.init_h = nn.Linear(3 * (self.feat_size + 1), self.hidden_size)\n",
    "\n",
    "    def embed_features(self, feats, targets):\n",
    "        batch_size = feats.shape[0]\n",
    "        n_obj = feats.shape[1]\n",
    "        rest = feats.shape[2:]\n",
    "        feats_flat = feats.view(batch_size * n_obj, *rest)\n",
    "        feats_emb_flat = self.feat_model(feats_flat)\n",
    "        feats_emb = feats_emb_flat.unsqueeze(1).view(batch_size, n_obj, -1)\n",
    "        # Add targets\n",
    "        #targets_onehot = to_onehot(targets)\n",
    "        targets_onehot = targets\n",
    "        feats_and_targets = torch.cat((feats_emb, targets_onehot.unsqueeze(2)), 2)\n",
    "        ft_concat = feats_and_targets.view(batch_size, -1)\n",
    "        return ft_concat\n",
    "\n",
    "    def forward(self, feats, targets, greedy=False, activation='gumbel', tau = 1, length_penalty=False, max_len=40):\n",
    "        \"\"\"Sample from image features\"\"\"\n",
    "        batch_size = feats.size(0)\n",
    "        feats_emb = self.embed_features(feats, targets)\n",
    "        # initialize hidden states using image features\n",
    "        states = self.init_h(feats_emb)\n",
    "        states = states.unsqueeze(0)\n",
    "        # This contains are series of sampled onehot vectors\n",
    "        lang = []\n",
    "        if length_penalty:\n",
    "            eos_prob = []\n",
    "        if activation == 'multinomial':\n",
    "            lang_prob = []\n",
    "        else:\n",
    "            lang_prob = None\n",
    "        # And vector lengths\n",
    "        lang_length = torch.ones(batch_size, dtype=torch.int64).to(feats.device)\n",
    "        done_sampling = [False for _ in range(batch_size)]\n",
    "        # first input is SOS token\n",
    "        # (batch_size, n_vocab)\n",
    "        inputs_onehot = torch.zeros(batch_size, self.vocab_size).to(feats.device)\n",
    "        inputs_onehot[:, SOS] = 1.0\n",
    "        # (batch_size, len, n_vocab)\n",
    "        inputs_onehot = inputs_onehot.unsqueeze(1)\n",
    "        # Add SOS to lang\n",
    "        lang.append(inputs_onehot)\n",
    "        # (B,L,D) to (L,B,D)\n",
    "        inputs_onehot = inputs_onehot.transpose(0, 1)\n",
    "        # compute embeddings\n",
    "        # (1, batch_size, n_vocab) X (n_vocab, h) -> (1, batch_size, h)\n",
    "        inputs = inputs_onehot @ self.embedding.weight\n",
    "\n",
    "        for i in range(max_len - 2):  # Have room for SOS, EOS if never sampled\n",
    "            # FIXME: This is inefficient since I do sampling even if we've\n",
    "            # finished generating language.\n",
    "            if all(done_sampling):\n",
    "                break\n",
    "            self.gru.flatten_parameters()\n",
    "            outputs, states = self.gru(inputs, states)  # outputs: (L=1,B,H)\n",
    "            outputs = outputs.squeeze(0)                # outputs: (B,H)\n",
    "            outputs = self.outputs2vocab(outputs)       # outputs: (B,V)\n",
    "            \n",
    "            if greedy:\n",
    "                predicted = outputs.max(1)[1]\n",
    "                predicted = predicted.unsqueeze(1)\n",
    "            else:\n",
    "                #  outputs = F.softmax(outputs, dim=1)\n",
    "                #  predicted = torch.multinomial(outputs, 1)\n",
    "                # TODO: Need to let language model accept one-hot vectors.\n",
    "                if activation=='gumbel'or activation==None:\n",
    "                    predicted_onehot = F.gumbel_softmax(outputs, tau=tau, hard=True)\n",
    "                elif activation=='softmax':\n",
    "                    predicted_onehot = F.softmax(outputs/tau)\n",
    "                elif activation=='softmax_noise':\n",
    "                    predicted_onehot = F.gumbel_softmax(outputs, tau=tau, hard=False)\n",
    "                elif activation == 'multinomial':\n",
    "                    # Normal non-differentiable sampling from the RNN, trained with REINFORCE\n",
    "                    TEMP = 5.0\n",
    "                    idx_prob = F.log_softmax(outputs / TEMP, dim=1)\n",
    "                    predicted = torch.multinomial(idx_prob.exp(), 1)\n",
    "                    predicted_onehot = to_onehot(predicted, n=self.vocab_size)\n",
    "                    predicted_logprob = torch.gather(idx_prob, 1, predicted)\n",
    "                    lang_prob.append(predicted_logprob)\n",
    "                else:\n",
    "                    raise NotImplementedError(activation)\n",
    "                    \n",
    "                # Add to lang\n",
    "                lang.append(predicted_onehot.unsqueeze(1))\n",
    "                if length_penalty:\n",
    "                    idx_prob = F.log_softmax(outputs, dim = 1)\n",
    "                    eos_prob.append(idx_prob[:,EOS])\n",
    "\n",
    "            predicted_npy = predicted_onehot.argmax(1).cpu().numpy()\n",
    "            \n",
    "            # Update language lengths\n",
    "            for j, pred in enumerate(predicted_npy):\n",
    "                if not done_sampling[j]:\n",
    "                    lang_length[j] += 1\n",
    "                if pred == EOS and activation in {'gumbel', 'multinomial'}:\n",
    "                    done_sampling[j] = True\n",
    "\n",
    "            # (1, batch_size, n_vocab) X (n_vocab, h) -> (1, batch_size, h)\n",
    "            inputs = (predicted_onehot.unsqueeze(0)) @ self.embedding.weight\n",
    "\n",
    "        # If multinomial, we need to run inputs once more to get the logprob of\n",
    "        # EOS (in case we've sampled that far)\n",
    "        if activation == 'multinomial':\n",
    "            self.gru.flatten_parameters()\n",
    "            outputs, states = self.gru(inputs, states)  # outputs: (L=1,B,H)\n",
    "            outputs = outputs.squeeze(0)                # outputs: (B,H)\n",
    "            outputs = self.outputs2vocab(outputs)       # outputs: (B,V)\n",
    "            idx_prob = F.log_softmax(outputs, dim=1)\n",
    "            lang_prob.append(idx_prob[:, EOS].unsqueeze(1))\n",
    "            \n",
    "        # Add EOS if we've never sampled it\n",
    "        eos_onehot = torch.zeros(batch_size, 1, self.vocab_size).to(feats.device)\n",
    "        eos_onehot[:, 0, EOS] = 1.0\n",
    "        lang.append(eos_onehot)\n",
    "        \n",
    "        # Cut off the rest of the sentences\n",
    "        for i, _ in enumerate(predicted_npy):\n",
    "            if not done_sampling[i]:\n",
    "                lang_length[i] += 1\n",
    "            done_sampling[i] = True\n",
    "\n",
    "        # Cat language tensors\n",
    "        lang_tensor = torch.cat(lang, 1)\n",
    "        \n",
    "        for i in range(lang_tensor.shape[0]):\n",
    "            lang_tensor[i, lang_length[i]:] = 0\n",
    "\n",
    "        # Trim max length\n",
    "        max_lang_len = lang_length.max()\n",
    "        lang_tensor = lang_tensor[:, :max_lang_len, :]\n",
    "        \n",
    "        if activation == 'multinomial':\n",
    "            lang_prob_tensor = torch.cat(lang_prob, 1)\n",
    "            for i in range(lang_prob_tensor.shape[0]):\n",
    "                lang_prob_tensor[i, lang_length[i]:] = 0\n",
    "            lang_prob_tensor = lang_prob_tensor[:, :max_lang_len]\n",
    "            lang_prob = lang_prob_tensor.sum(1)\n",
    "        \n",
    "        if length_penalty:\n",
    "            # eos prob -> eos loss\n",
    "            eos_prob = torch.stack(eos_prob, dim = 1)\n",
    "            for i in range(eos_prob.shape[0]):\n",
    "                r_len = torch.arange(1,eos_prob.shape[1]+1,dtype=torch.float32)\n",
    "                eos_prob[i] = eos_prob[i]*r_len.to(eos_prob.device)\n",
    "                eos_prob[i, lang_length[i]:] = 0\n",
    "            eos_loss = -eos_prob\n",
    "            eos_loss = eos_loss.sum(1)/lang_length.float()\n",
    "            eos_loss = eos_loss.mean()\n",
    "        else:\n",
    "            eos_loss = 0\n",
    "            \n",
    "        # Sum up log probabilities of samples\n",
    "        return lang_tensor, lang_length, eos_loss, lang_prob\n",
    "\n",
    "    def to_text(self, lang_onehot):\n",
    "        texts = []\n",
    "        lang = lang_onehot.argmax(2)\n",
    "        for sample in lang.cpu().numpy():\n",
    "            text = []\n",
    "            for item in sample:\n",
    "                text.append(i2w[item])\n",
    "                if item == EOS:\n",
    "                    break\n",
    "            texts.append(' '.join(text))\n",
    "        return np.array(texts, dtype=np.unicode_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0\n",
    "from vision import ConvNet\n",
    "\n",
    "emb_dim = 768\n",
    "speaker_embs = nn.Embedding(len(w2i), emb_dim)\n",
    "#speaker_feat = Imgs_Feature(output_size=1024)\n",
    "speaker_feat = ConvNet(4)\n",
    "speaker = Speaker(speaker_feat, speaker_embs)\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "max_len = 4\n",
    "\n",
    "optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losss = []\n",
    "accs = []\n",
    "\n",
    "for i,(cols,label,lang) in enumerate(train_batch):\n",
    "    cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "    label = label.to(device).to(torch.float)\n",
    "    optimizer.zero_grad()\n",
    "    literal_listener.train()\n",
    "    speaker.train()\n",
    "    lang_tensor,lang_length,eos_loss,lang_prob = speaker(cols, label, length_penalty=False, max_len=max_len)\n",
    "    #print(lang_tensor.shape)\n",
    "    #print(lang_length.shape)\n",
    "    #print(eos_loss)\n",
    "    #print(lang_prob.shape)\n",
    "    #print(lang.shape)\n",
    "    lang_out = lang_tensor.view(lang_tensor.size(0)*lang_tensor.size(1), len(vocab['w2i'].keys()))\n",
    "    lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(w2i.keys())).unsqueeze(0) for sent in lang]))\n",
    "    #print(lang_onehot.shape)\n",
    "    lang_in = lang_onehot.long().view(lang.size(0)*lang.size(1), len(vocab['w2i'].keys()))\n",
    "    #print(lang_in.shape)\n",
    "    #print(lang_out.shape)\n",
    "    #print(torch.max(lang_in, 1)[1].shape)\n",
    "    lang_loss = criterion(lang_out.cuda(), torch.max(lang_in, 1)[1].cuda())\n",
    "    output_lang = lang_tensor.argmax(2)\n",
    "    lis_labels = literal_listener(cols, output_lang)\n",
    "    loss = criterion(lis_labels,label) + lang_loss + eos_loss*0.0001\n",
    "    #loss = criterion(lang_tensor.to(torch.float),lang.to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%10 == 0:\n",
    "        #print(lis_labels[0])\n",
    "        #print(label[0])\n",
    "        print(\"\\nOriginal sentence:\\n\"+\" \".join([i2w[idx] for idx in lang[0].to(\"cpu\").tolist()]).replace(\" <pad>\",\"\"))\n",
    "        print(\"Generated sentence:\\n\"+\" \".join([i2w[idx] for idx in lang_tensor.argmax(2)[0].to(\"cpu\").tolist()]).replace(\" <pad>\",\"\")+\"\\n\")\n",
    "        print(\"Loss: \",loss.item())\n",
    "        losss.append(loss.item())\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(cols.shape[0])+2\n",
    "        acc = (sum(correct_labels.to(device)==pred_labels)/len(correct_labels)).item()\n",
    "        print(\"Accuracy:\",acc)\n",
    "        accs.append(acc)\n",
    "    if i > 100: break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0\n",
    "from vision import ConvNet\n",
    "\n",
    "emb_dim = 768\n",
    "speaker_embs = nn.Embedding(len(w2i), emb_dim)\n",
    "#speaker_feat = Imgs_Feature(output_size=1024)\n",
    "speaker_feat = ConvNet(4)\n",
    "speaker = Speaker(speaker_feat, speaker_embs)\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "max_len = 4\n",
    "\n",
    "optimizer = optim.Adam(speaker.parameters(),lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "best_loss = 100\n",
    "for i in range(epoch):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    # freeze the weights and L0.train for RNN loss backward\n",
    "    #for weights in literal_listener.parameters():\n",
    "    #    weights.requires_grad = False\n",
    "    literal_listener.train()\n",
    "    speaker.train()\n",
    "    #print(\"Start Training\")\n",
    "    for cols,label,lang in train_batch:\n",
    "        cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "        label = label.to(device).to(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        lang_tensor,lang_length,eos_loss,lang_prob = speaker(cols, label, length_penalty=False, max_len=max_len)\n",
    "        # for L1 loss\n",
    "        output_lang = lang_tensor.argmax(2)\n",
    "        # prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] for batch,(sent,idxs) in enumerate(zip(lang_tensor,output_lang))]\n",
    "        # prob01_sums = list(map(sum,prob01))\n",
    "        # label02 = torch.zeros_like(label)\n",
    "        # label02[:,1] = 1.0\n",
    "        # lang_tensor1,lang_length,eos_loss,lang_prob = speaker(cols, label02, length_penalty=False, max_len=max_len)\n",
    "        # prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] for batch,(sent,idxs) in enumerate(zip(lang_tensor1,output_lang))]\n",
    "        # prob02_sums = list(map(sum,prob02))\n",
    "        # label03 = torch.zeros_like(label)\n",
    "        # label03[:,2] = 1.0\n",
    "        # lang_tensor2,lang_length,eos_loss,lang_prob = speaker(cols, label03, length_penalty=False, max_len=max_len)\n",
    "        # prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] for batch,(sent,idxs) in enumerate(zip(lang_tensor2,output_lang))]\n",
    "        # prob03_sums = list(map(sum,prob03))\n",
    "        # probs = torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1)\n",
    "        # l1_loss = criterion(probs.to(device),label) + eos_loss*0.0001\n",
    "        # for L0 loss\n",
    "        lis_labels = literal_listener(cols, output_lang).to(device)\n",
    "        l0_loss = criterion(lis_labels,label)\n",
    "        #loss = l0_loss + l1_loss + eos_loss*0.0001\n",
    "        loss = l0_loss + eos_loss*0.0001\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(pred_labels.shape[0])+2\n",
    "        train_acc += sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    batch_train_acc = train_acc/len(train_batch)\n",
    "\n",
    "    speaker.eval()\n",
    "    #print(\"Start Evaluation\")\n",
    "    for cols,label,lang in test_batch:\n",
    "        cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "        label = label.to(device).to(torch.float)\n",
    "        lang_tensor,lang_length,eos_loss,lang_prob = speaker(cols, label, length_penalty=False, max_len=max_len)\n",
    "        # for L1 loss\n",
    "        output_lang = lang_tensor.argmax(2)\n",
    "        # prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] for batch,(sent,idxs) in enumerate(zip(lang_tensor,output_lang))]\n",
    "        # prob01_sums = list(map(sum,prob01))\n",
    "        # label02 = torch.zeros_like(label)\n",
    "        # label02[:,1] = 1.0\n",
    "        # lang_tensor1,lang_length,eos_loss,lang_prob = speaker(cols, label02, length_penalty=False, max_len=max_len)\n",
    "        # prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] for batch,(sent,idxs) in enumerate(zip(lang_tensor1,output_lang))]\n",
    "        # prob02_sums = list(map(sum,prob02))\n",
    "        # label03 = torch.zeros_like(label)\n",
    "        # label03[:,2] = 1.0\n",
    "        # lang_tensor2,lang_length,eos_loss,lang_prob = speaker(cols, label03, length_penalty=False, max_len=max_len)\n",
    "        # prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] for batch,(sent,idxs) in enumerate(zip(lang_tensor2,output_lang))]\n",
    "        # prob03_sums = list(map(sum,prob03))\n",
    "        # probs = torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1)\n",
    "        # l1_loss = criterion(probs.to(device),label) + eos_loss*0.0001\n",
    "        # for L0 loss\n",
    "        lis_labels = literal_listener(cols, output_lang).to(device)\n",
    "        l0_loss = criterion(lis_labels,label)\n",
    "        #loss = l0_loss + l1_loss + eos_loss*0.0001\n",
    "        loss = l0_loss + eos_loss*0.0001\n",
    "        test_loss += loss.item()\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(pred_labels.shape[0])+2\n",
    "        test_acc += (sum(correct_labels.to(device)==pred_labels)/len(correct_labels)).item()\n",
    "    batch_test_loss = test_loss/len(test_batch)\n",
    "    batch_test_acc = test_acc/len(test_batch)\n",
    "\n",
    "    print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "    print(\"Train Acc:{:.2E}, Test Acc:{:.2E}\".format(batch_train_acc,batch_test_acc))\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    test_loss_list.append(batch_test_loss)\n",
    "    train_acc_list.append(batch_train_acc)\n",
    "    test_acc_list.append(batch_test_acc)\n",
    "    if batch_test_loss < best_loss:\n",
    "        # save\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/color_S1_lis=emb-rnn-L0_original_rnn_no-penalty_L0-loss.pth\")\n",
    "        best_loss = batch_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure()\n",
    "plt.title(\"Train and Test Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Literal_Listener_loss\")\n",
    "plt.plot(range(1,epoch+1),train_loss_list,\"b-\",label=\"train_loss\")\n",
    "plt.plot(range(1,epoch+1),test_loss_list,\"r--\",label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "train_acc_list = [float(acc) for acc in train_acc_list]\n",
    "test_acc_list = [float(acc) for acc in test_acc_list]\n",
    "plt.figure()\n",
    "plt.title(\"Train and Test Accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,epoch+1),train_acc_list,\"b-\",label=\"train_acc\")\n",
    "plt.plot(range(1,epoch+1),test_acc_list,\"r--\",label=\"test_acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[2]))\n",
    "print(d[\"imgs\"].shape)\n",
    "print(d[\"labels\"].shape)\n",
    "print(d[\"langs\"].shape)\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"],id=1)\n",
    "eval_batch = DataLoader(ShapeWorld(d, vocab), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def check_data(imgs, label, g_langs, c_langs):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 4))\n",
    "    title = re.sub(r\"<sos>|<eos>\",\"\",\"Generated: \"+\" \".join(g_langs)+\"\\n\\nCorrect: \"+\" \".join(c_langs))\n",
    "    fig.suptitle(title)\n",
    "    for i,(l,img) in enumerate(zip(label,imgs)):\n",
    "        img = img.transpose(2,0)\n",
    "        axes[i].imshow(img)\n",
    "        if l==1: axes[i].set_title(\"Correct\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker_embs = nn.Embedding(len(w2i), emb_dim)\n",
    "#speaker_feat = Imgs_Feature(output_size=1024)\n",
    "#speaker_feat = ConvNet(4)\n",
    "#speaker = Speaker(speaker_feat, speaker_embs)\n",
    "#speaker.load_state_dict(torch.load(\"model_params\\color_S1_lis=emb-rnn-L0_original_rnn_no-penalty+cpu-lis.pth\",map_location=device))\n",
    "#speaker.to(device)\n",
    "\n",
    "losss = []\n",
    "accs = []\n",
    "for i,(cols,label,lang) in enumerate(eval_batch):\n",
    "    cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "    label = label.to(device).to(torch.float)\n",
    "    literal_listener.eval()\n",
    "    speaker.eval()\n",
    "    lang_tensor,lang_length,eos_loss,lang_prob = speaker(cols, label, length_penalty=False, max_len=max_len)\n",
    "    output_lang = lang_tensor.argmax(2)\n",
    "    lis_labels = literal_listener(cols, output_lang)\n",
    "    loss = criterion(lis_labels,label) + eos_loss*0.0001\n",
    "    if i%10 == 0:\n",
    "        imgs = cols[0].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[0].to(\"cpu\").tolist()]\n",
    "        g_langs = [i2w[idx] for idx in lang_tensor.argmax(2)[0].to(\"cpu\").tolist()]\n",
    "        label = label[0]\n",
    "        check_data(imgs, label, g_langs, c_langs)\n",
    "        #print(\"Loss: \",loss.item())\n",
    "        losss.append(loss.item())\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(cols.shape[0])\n",
    "        acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "        #print(\"Accuracy:\",acc)\n",
    "        accs.append(acc.item())\n",
    "    if i > 100: break\n",
    "\n",
    "print(\"Accuracy:,\",np.mean(accs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss = []\n",
    "accs = []\n",
    "for i,(cols,label,lang) in enumerate(eval_batch):\n",
    "    cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "    label = label.to(device).to(torch.float)\n",
    "    literal_listener.eval()\n",
    "    speaker.eval()\n",
    "    lang_tensor,lang_length,eos_loss,lang_prob = speaker(cols, label, length_penalty=False, max_len=max_len)\n",
    "    output_lang1 = lang_tensor.argmax(2)\n",
    "    # for 2nd image\n",
    "    label02 = torch.zeros_like(label)\n",
    "    label02[:,1] = 1.0\n",
    "    lang_tensor1,lang_length,eos_loss,lang_prob = speaker(cols, label02, length_penalty=False, max_len=max_len)\n",
    "    output_lang2 = lang_tensor1.argmax(2)\n",
    "    # for 3rd image\n",
    "    label03 = torch.zeros_like(label)\n",
    "    label03[:,2] = 1.0\n",
    "    lang_tensor2,lang_length,eos_loss,lang_prob = speaker(cols, label03, length_penalty=False, max_len=max_len)\n",
    "    output_lang3 = lang_tensor2.argmax(2)\n",
    "    \n",
    "    prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_tensor,output_lang3))]\n",
    "    prob01_sums = list(map(sum,prob01))\n",
    "    prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_tensor1,output_lang3))]\n",
    "    prob02_sums = list(map(sum,prob02))\n",
    "    prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_tensor2,output_lang3))]\n",
    "    prob03_sums = list(map(sum,prob03))\n",
    "    probs = torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1)\n",
    "    #print(probs)\n",
    "    loss = criterion(probs.to(device),label) + eos_loss*0.0001\n",
    "    losss.append(loss.item())\n",
    "    pred_labels = torch.argmax(probs,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])+2\n",
    "    acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "    accs.append(acc.item())\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        imgs = cols[0].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[0].to(\"cpu\").tolist()]\n",
    "        g_langs = [i2w[idx] for idx in output_lang[0].to(\"cpu\").tolist()] \\\n",
    "                + [\"|\"]+ [i2w[idx] for idx in output_lang2[0].to(\"cpu\").tolist()] \\\n",
    "                + [\"|\"]+ [i2w[idx] for idx in output_lang3[0].to(\"cpu\").tolist()]\n",
    "        label = label[0]\n",
    "        check_data(imgs, label, g_langs, c_langs)\n",
    "        print(torch.exp(probs[0]))\n",
    "        print(torch.where(torch.exp(probs[0])>0.1,1,0))\n",
    "        #print(\"Loss: \",loss.item())\n",
    "    if i > 100: break\n",
    "\n",
    "print(\"Accuracy:,\",np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
