{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from shapeworld_data import load_raw_data, get_vocab, ShapeWorld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_raw_data(imgs, labels, langs, id=0):\n",
    "    data = list(zip(imgs,labels,langs))\n",
    "    img_list,label,lang = data[id]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 2))\n",
    "    fig.suptitle(\" \".join(lang))\n",
    "    for i,(l,img) in enumerate(zip(label,img_list)):\n",
    "        img = img.transpose((2,1,0))\n",
    "        axes[i].imshow(img)\n",
    "        if l==1: axes[i].set_title(\"Correct\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.path.abspath('')).parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\\shapeworld_np\")\n",
    "print(data_path)\n",
    "data_list = os.listdir(data_path)\n",
    "print(data_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vocab_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab([os.path.join(data_path,d) for d in data_list])\n",
    "print(vocab[\"w2i\"])\n",
    "\n",
    "COLOR = {\"white\":[1,0,0,0,0,0], \"green\":[0,1,0,0,0,0], \"gray\":[0,0,1,0,0,0], \"yellow\":[0,0,0,1,0,0], \"red\":[0,0,0,0,1,0], \"blue\":[0,0,0,0,0,1], \"other\":[0,0,0,0,0,0]}\n",
    "SHAPE = {\"shape\":[0,0,0,0], \"square\":[1,0,0,0], \"circle\":[0,1,0,0], \"rectangle\":[0,0,1,0], \"ellipse\":[0,0,0,1]}\n",
    "\n",
    "w2i = vocab[\"w2i\"]\n",
    "i2w = vocab[\"i2w\"]\n",
    "\n",
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def utter2tensor(utter):\n",
    "    utter = re.sub(r\"<sos>|<eos>|<PAD>|<UNK>\",\"\",utter)\n",
    "    #print(utter)\n",
    "    utters = utter.split(\" \")\n",
    "    if len(utters) == 1 and utters[0] in COLOR.keys():\n",
    "        return torch.tensor(np.array(COLOR[utters[0]]+SHAPE[\"shape\"]))\n",
    "    elif len(utters) == 1 and utters[0] in SHAPE.keys():\n",
    "        return torch.tensor(np.array(COLOR[\"other\"]+SHAPE[utters[0]]))\n",
    "    elif len(utters) == 2:\n",
    "        return torch.tensor(np.array(COLOR[utters[0]]+SHAPE[utters[1]]))\n",
    "    else:\n",
    "        return torch.tensor(np.array(COLOR[\"other\"]+SHAPE[\"shape\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(COLOR.keys())\n",
    "col_list[-1] = \"\"\n",
    "shape_list = list(SHAPE.keys())\n",
    "utter_list = [\" \".join([w for w in (c+\" \"+s).split(\" \") if w]) for c in col_list for s in shape_list+[\"\"]]\n",
    "for l in utter_list:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def lang2class(utter):\n",
    "    utter = \" \".join([w for w in re.sub(r\"<sos>|<eos>|<PAD>|<UNK>\",\"\",utter).split(\" \") if w])\n",
    "    #print(utter)\n",
    "    onehot = torch.zeros(len(utter_list))\n",
    "    onehot[utter_list.index(utter)] = 1.0\n",
    "    return (onehot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[0]))\n",
    "imgs = d[\"imgs\"]\n",
    "labels = d[\"labels\"]\n",
    "langs = d[\"langs\"]\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"])\n",
    "for i in range(1,4):\n",
    "    d = load_raw_data(os.path.join(data_path,data_list[i]))\n",
    "    imgs = np.vstack((imgs,d[\"imgs\"]))\n",
    "    labels = np.vstack((labels,d[\"labels\"]))\n",
    "    langs = np.hstack((langs,d[\"langs\"]))\n",
    "    check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"])\n",
    "d[\"imgs\"] = imgs\n",
    "d[\"labels\"] = labels\n",
    "d[\"langs\"] = langs\n",
    "\n",
    "all_in = []\n",
    "for l in d[\"langs\"]:\n",
    "    lang = \" \".join(l)\n",
    "    all_in.append(lang in utter_list)\n",
    "print(\"All lang in list: \",all(all_in))\n",
    "\n",
    "print(d[\"imgs\"].shape, d[\"labels\"].shape, d[\"langs\"].shape)\n",
    "print(len(ShapeWorld(d, vocab)))\n",
    "train_batch = DataLoader(ShapeWorld(d, vocab), batch_size=64, shuffle=False)\n",
    "\n",
    "all_classified = []\n",
    "for cols,label,lang in train_batch:\n",
    "    for i in range(len(cols)):\n",
    "        utter = \" \".join([i2w[int(idx)] for idx in lang[i]])\n",
    "        try:\n",
    "            vec = lang2class(utter)\n",
    "            all_classified.append(True)\n",
    "        except:\n",
    "            print(utter,\" not in list\")\n",
    "print(\"All lang classified: \",all(all_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[-1]))\n",
    "print(d[\"imgs\"].shape)\n",
    "print(d[\"labels\"].shape)\n",
    "print(d[\"langs\"].shape)\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"],id=1)\n",
    "all_in = []\n",
    "for l in d[\"langs\"]:\n",
    "    lang = \" \".join(l)\n",
    "    all_in.append(lang in utter_list)\n",
    "print(\"All lang in list: \",all(all_in))\n",
    "test_batch = DataLoader(ShapeWorld(d, vocab), batch_size=32, shuffle=False)\n",
    "\n",
    "all_classified = []\n",
    "for cols,label,lang in test_batch:\n",
    "    for i in range(len(cols)):\n",
    "        utter = \" \".join([i2w[int(idx)] for idx in lang[i]])\n",
    "        try:\n",
    "            vec = lang2class(utter)\n",
    "            all_classified.append(True)\n",
    "        except:\n",
    "            print(utter,\" not in list\")\n",
    "print(\"All lang classified: \",all(all_classified))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imgs_emb_DeepSet(nn.Module):\n",
    "    def __init__(self, input_size=10, output_size=20):\n",
    "        super(Imgs_emb_DeepSet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, output_size)\n",
    "        self.linear2 = nn.Linear(input_size, output_size)\n",
    "        self.linear3 = nn.Linear(output_size, output_size)\n",
    "    \n",
    "    def forward(self, img_emb1, img_emb2):\n",
    "        img_embs = F.relu(self.linear1(img_emb1)) + F.relu(self.linear2(img_emb2))\n",
    "        img_embs = self.linear3(img_embs)\n",
    "        return img_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import CNN_encoder\n",
    "\n",
    "class CS_CNN_encoder_Feature(nn.Module):\n",
    "    def __init__(self, input_size=10, output_size=10):\n",
    "        super(CS_CNN_encoder_Feature, self).__init__()\n",
    "        self.cnn_color_encoder = CNN_encoder(6)\n",
    "        self.cnn_color_encoder.load_state_dict(torch.load(\"model_params/cnn_color_model_best-loss.pth\",map_location=device))\n",
    "        for params in self.cnn_color_encoder.parameters(): params.requires_grad = False\n",
    "        self.cnn_shape_encoder = CNN_encoder(4)\n",
    "        self.cnn_shape_encoder.load_state_dict(torch.load(\"model_params/cnn_shape_model_best-loss.pth\",map_location=device))\n",
    "        for params in self.cnn_shape_encoder.parameters(): params.requires_grad = False\n",
    "        self.deepset_size = 6+4\n",
    "        self.deepset = Imgs_emb_DeepSet(self.deepset_size, self.deepset_size*2)\n",
    "\n",
    "    def get_feat_emb(self,feat):\n",
    "        col_embs = self.cnn_color_encoder(feat)\n",
    "        shape_embs = self.cnn_shape_encoder(feat)\n",
    "        img_embs = torch.hstack((col_embs,shape_embs))\n",
    "        return img_embs\n",
    "    \n",
    "    def forward(self,feats,labels):\n",
    "        idxs = [0,1,2]\n",
    "        target_idx = int(torch.argmax(labels))\n",
    "        idxs.remove(target_idx)\n",
    "        other_idx1,other_idx2 = idxs[0],idxs[1]\n",
    "        target_img,other_img1,other_img2 = feats[:,target_idx], feats[:,other_idx1], feats[:,other_idx2]\n",
    "        target_embs = self.get_feat_emb(target_img)\n",
    "        other_embs1 = self.get_feat_emb(other_img1)\n",
    "        other_embs2 = self.get_feat_emb(other_img2)                 # (batch_size,10)\n",
    "        other_embs = self.deepset(other_embs1,other_embs2)          # (batch_size,20)\n",
    "        feat = torch.hstack((target_embs,other_embs,labels))        # (batch_size,33)\n",
    "        #feat = self.linear(embs)                                    # (batch_size,output_size)\n",
    "        return feat,target_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "    def __init__(self, feat_model, feat_size, output_size, hidden_size=100):\n",
    "        super(Speaker, self).__init__()\n",
    "        self.feat_model = feat_model\n",
    "        self.linear1 = nn.Linear(feat_size,hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, feats, labels):\n",
    "        feats_emb,target_embs = self.feat_model(feats,labels)\n",
    "        x = F.relu(self.linear1(feats_emb))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        y_prob = F.softmax(self.linear3(x),dim=-1)\n",
    "        return y_prob,target_embs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch(cols,langs,label,onehot_classs,target,target2,id=0):\n",
    "    data = list(zip(cols,langs,label,onehot_classs,target,target2))\n",
    "    img_list,lang,label,onehot_class,target,target2 = data[id]\n",
    "    lang = \" \".join([i2w[int(idx)] for idx in lang])\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 4))\n",
    "    fig.suptitle(\" \".join(lang)+\"\\nClassified: \"+utter_list[int(torch.argmax(onehot_class))]+\"\\nCNN insode model: \"+target+\"\\nFreezed CNN: \"+target2)\n",
    "    for i,(l,img) in enumerate(zip(label,img_list)):\n",
    "        img = img.transpose(2,0).to(\"cpu\").detach().numpy()\n",
    "        axes[i].imshow(img)\n",
    "        if l==1: axes[i].set_title(\"Correct\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0\n",
    "\n",
    "emb_dim = 768\n",
    "feat_dim = 10\n",
    "output_dim = len(utter_list)\n",
    "speaker_feat = CS_CNN_encoder_Feature(input_size=feat_dim,output_size=feat_dim)\n",
    "speaker = Speaker(speaker_feat, feat_size=feat_dim*3+3,output_size=output_dim)\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "cnn_color_encoder = CNN_encoder(6).to(device)\n",
    "cnn_color_encoder.load_state_dict(torch.load(\"model_params/cnn_color_model_best-loss.pth\",map_location=device))\n",
    "for params in cnn_color_encoder.parameters(): params.requires_grad = False\n",
    "cnn_shape_encoder = CNN_encoder(4).to(device)\n",
    "cnn_shape_encoder.load_state_dict(torch.load(\"model_params/cnn_shape_model_best-loss.pth\",map_location=device))\n",
    "for params in cnn_shape_encoder.parameters(): params.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(speaker.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "best_loss = 100\n",
    "best_acc = 0\n",
    "for i in range(epoch):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    literal_listener.train()\n",
    "    speaker.train()\n",
    "    #print(\"Start Training\")\n",
    "    for cols,label,lang in train_batch:\n",
    "        cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "        label = label.to(device).to(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        y_prob,target = speaker(cols, label)\n",
    "        # from model\n",
    "        target_col, target_shape = target[:,:6], target[:,6:]\n",
    "        target_cols = [list(COLOR.keys())[int(torch.argmax(col_vec))] for col_vec in target_col]\n",
    "        target_shapes = [list(SHAPE.keys())[int(torch.argmax(shape_vec))] for shape_vec in target_shape]\n",
    "        target_utter = [c+\" \"+s for c,s in zip(target_cols,target_shapes)]\n",
    "        # from encoder directly\n",
    "        target_col, target_shape = cnn_color_encoder(cols[:,0]), cnn_shape_encoder(cols[:,0])\n",
    "        target_cols = [list(COLOR.keys())[int(torch.argmax(col_vec))] for col_vec in target_col]\n",
    "        target_shapes = [list(SHAPE.keys())[int(torch.argmax(shape_vec))] for shape_vec in target_shape]\n",
    "        direct_target_utter = [c+\" \"+s for c,s in zip(target_cols,target_shapes)]\n",
    "        # for classification task\n",
    "        class_label = torch.vstack(tuple([lang2class(\" \".join([i2w[int(idx)] for idx in l])) for l in lang])).to(device)\n",
    "        clas_loss = criterion(y_prob, class_label.to(device))\n",
    "        # for literal lisner loss\n",
    "        #lis_labels = literal_listener(cols, output_lang)\n",
    "        #lis_loss = criterion(lis_labels,label)\n",
    "        loss = clas_loss #+ lis_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        #correct_labels = torch.zeros(pred_labels.shape[0])+2\n",
    "        #train_acc += sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "        train_acc += sum(torch.argmax(class_label,dim=1)==torch.argmax(y_prob,dim=1))/len(class_label)\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    batch_train_acc = train_acc/len(train_batch)\n",
    "\n",
    "    speaker.eval()\n",
    "    #print(\"Start Evaluation\")\n",
    "    for cols,label,lang in test_batch:\n",
    "        cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "        label = label.to(device).to(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        y_prob,target = speaker(cols, label)\n",
    "        # from model\n",
    "        target_col, target_shape = target[:,:6], target[:,6:]\n",
    "        target_cols = [list(COLOR.keys())[int(torch.argmax(col_vec))] for col_vec in target_col]\n",
    "        target_shapes = [list(SHAPE.keys())[int(torch.argmax(shape_vec))] for shape_vec in target_shape]\n",
    "        target_utter = [c+\" \"+s for c,s in zip(target_cols,target_shapes)]\n",
    "        # from encoder directly\n",
    "        target_col, target_shape = cnn_color_encoder(cols[:,0]), cnn_shape_encoder(cols[:,0])\n",
    "        target_cols = [list(COLOR.keys())[int(torch.argmax(col_vec))] for col_vec in target_col]\n",
    "        target_shapes = [list(SHAPE.keys())[int(torch.argmax(shape_vec))] for shape_vec in target_shape]\n",
    "        direct_target_utter = [c+\" \"+s for c,s in zip(target_cols,target_shapes)]\n",
    "        # for classification task\n",
    "        class_label = torch.vstack(tuple([lang2class(\" \".join([i2w[int(idx)] for idx in l])) for l in lang])).to(device)\n",
    "        loss = criterion(y_prob, class_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        test_loss += loss.item()\n",
    "        test_acc += sum(torch.argmax(class_label,dim=1)==torch.argmax(y_prob,dim=1))/len(class_label)\n",
    "    batch_test_loss = test_loss/len(test_batch)\n",
    "    batch_test_acc = test_acc/len(test_batch)\n",
    "    \n",
    "    if i%1==0: check_batch(cols,lang,label,y_prob,target_utter,direct_target_utter,id=np.random.randint(len(cols)))\n",
    "    print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "    print(\"Train Acc:{:.2E}, Test Acc:{:.2E}\".format(batch_train_acc,batch_test_acc))\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    test_loss_list.append(batch_test_loss)\n",
    "    train_acc_list.append(batch_train_acc)\n",
    "    test_acc_list.append(batch_test_acc)\n",
    "    if batch_test_loss < best_loss:\n",
    "        print(\"Best loss saved ...\")\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/shapeworld_S1-class-ext-deep-dim-with-label_lis=emb-rnn_CS-CNN-encoder_more-data_best_loss.pth\")\n",
    "        best_loss = batch_test_loss\n",
    "    if batch_test_acc > best_acc:\n",
    "        print(\"Best acc saved ...\")\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/shapeworld_S1-class-ext-deep-dim-with-label_lis=emb-rnn_CS-CNN-encoder_more-data_best_acc.pth\")\n",
    "        best_acc = batch_test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "plt.figure()\n",
    "plt.title(\"Train and Test Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Literal_Listener_loss\")\n",
    "plt.plot(range(1,epoch+1),train_loss_list,\"b-\",label=\"train_loss\")\n",
    "plt.plot(range(1,epoch+1),test_loss_list,\"r--\",label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "train_acc_list = [float(acc) for acc in train_acc_list]\n",
    "test_acc_list = [float(acc) for acc in test_acc_list]\n",
    "plt.figure()\n",
    "plt.title(\"Train and Test Accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,epoch+1),train_acc_list,\"b-\",label=\"train_acc\")\n",
    "plt.plot(range(1,epoch+1),test_acc_list,\"r--\",label=\"test_acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(speaker.to(device).state_dict(),\"model_params/shapeworld_S1_class_final-30-epoch.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[-1]))\n",
    "print(d[\"imgs\"].shape)\n",
    "print(d[\"labels\"].shape)\n",
    "print(d[\"langs\"].shape)\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"],id=1)\n",
    "eval_batch = DataLoader(ShapeWorld(d, vocab), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def check_data(imgs, label, g_langs, c_langs, cap=\"Correct\"):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 4))\n",
    "    title = re.sub(r\"<sos>|<eos>\",\"\",\"Generated: \"+\" \".join(g_langs)+\"\\n\\nCorrect: \"+\" \".join(c_langs))\n",
    "    fig.suptitle(title)\n",
    "    t_idx = int(torch.argmax(label))\n",
    "    for i,(l,img) in enumerate(zip(label,imgs)):\n",
    "        img = img.transpose(2,0)\n",
    "        axes[i].imshow(img)\n",
    "        if t_idx==i: axes[i].set_title(cap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker.load_state_dict(torch.load(\"model_params/shapeworld_S1-class-ext-deep-dim-with-label_lis=emb-rnn_CS-CNN-encoder_more-data_best_acc.pth\",map_location=device))\n",
    "speaker.to(device)\n",
    "print(\"model loaded successfully\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker.eval()\n",
    "\n",
    "losss = []\n",
    "accs = []\n",
    "for i,(cols,label,lang) in enumerate(eval_batch):\n",
    "    cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "    label = label.to(device).to(torch.float)\n",
    "    y_prob,target = speaker(cols, label)\n",
    "    class_label = torch.vstack(tuple([lang2class(\" \".join([i2w[int(idx)] for idx in l])) for l in lang])).to(device)\n",
    "    acc = (sum(torch.argmax(class_label,dim=1)==torch.argmax(y_prob,dim=1))/len(class_label)).item()\n",
    "    accs.append(acc)\n",
    "    if i%10 == 0:\n",
    "        imgs = cols[0].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[0].to(\"cpu\").tolist()]\n",
    "        g_langs = utter_list[int(torch.argmax(y_prob[0]))].split(\" \")\n",
    "        label = label[0]\n",
    "        check_data(imgs, label, g_langs, c_langs)\n",
    "\n",
    "print(\"Accuracy:,\",np.mean(accs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_2word(y_vec):\n",
    "    utters = utter_list[int(torch.argmax(y_vec))].split(\" \")\n",
    "    if len(utters) == 2:\n",
    "        return [w2i[w] for w in ([\"<sos>\"]+utters+[\"<eos>\"])]\n",
    "    else:\n",
    "        return [w2i[w] for w in ([\"<sos>\"]+utters+[\"<PAD>\",\"<eos>\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literal_listener.to(device)\n",
    "literal_listener.eval()\n",
    "speaker.eval()\n",
    "\n",
    "losss = []\n",
    "accs = []\n",
    "for i,(cols,label,lang) in enumerate(eval_batch):\n",
    "    cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "    label = label.to(device).to(torch.float)\n",
    "    y_prob,target = speaker(cols, label)\n",
    "    output_lang = torch.tensor(np.array([onehot_to_2word(y) for y in y_prob])).to(device)\n",
    "    #print(output_lang.shape)\n",
    "    lis_labels = literal_listener(cols, output_lang)\n",
    "    loss = criterion(lis_labels,label)\n",
    "    #print(\"Loss: \",loss.item())\n",
    "    losss.append(loss.item())\n",
    "    pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])\n",
    "    acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "    #print(\"Accuracy:\",acc)\n",
    "    accs.append(acc.item())\n",
    "    if i%10 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        imgs = cols[id].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[id].to(\"cpu\").tolist()]\n",
    "        g_langs = [i2w[idx] for idx in output_lang[id].to(\"cpu\").tolist()]\n",
    "        label = label[0]\n",
    "        lis_label = lis_labels[id]\n",
    "        print(lis_label.to(\"cpu\").tolist())\n",
    "        check_data(imgs, lis_label, g_langs, c_langs, cap=\"L0 prediction\")\n",
    "\n",
    "print(\"Accuracy:,\",np.mean(accs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langTensor2idx(langt):\n",
    "    utter = \" \".join([i2w[int(idx)] for idx in langt])\n",
    "    utter = \" \".join([w for w in re.sub(r\"<sos>|<eos>|<PAD>|<UNK>\",\"\",utter).split(\" \") if w])\n",
    "    #print(utter)\n",
    "    return utter_list.index(utter)\n",
    "\n",
    "losss = []\n",
    "accs = []\n",
    "for i,(cols,label,lang) in enumerate(eval_batch):\n",
    "    cols, lang = cols.to(device).to(torch.float), lang.to(device)\n",
    "    label = label.to(device).to(torch.float)\n",
    "    literal_listener.eval()\n",
    "    speaker.eval()\n",
    "    y_prob1,target = speaker(cols, label)\n",
    "    output_lang = torch.tensor(np.array([onehot_to_2word(y) for y in y_prob1])).to(device)\n",
    "    # for 2nd image\n",
    "    label02 = torch.zeros_like(label)\n",
    "    label02[:,1] = 1.0\n",
    "    y_prob2,target = speaker(cols, label02)\n",
    "    output_lang2 = torch.tensor(np.array([onehot_to_2word(y) for y in y_prob2])).to(device)\n",
    "    # for 3rd image\n",
    "    label03 = torch.zeros_like(label)\n",
    "    label03[:,2] = 1.0\n",
    "    y_prob3,target = speaker(cols, label03)\n",
    "    output_lang3 = torch.tensor(np.array([onehot_to_2word(y) for y in y_prob3])).to(device)\n",
    "    \n",
    "    prob01 = [y[langTensor2idx(l)].to(\"cpu\").detach() for y,l in zip(y_prob1,lang)]\n",
    "    prob02 = [y[langTensor2idx(l)].to(\"cpu\").detach() for y,l in zip(y_prob2,lang)]\n",
    "    prob03 = [y[langTensor2idx(l)].to(\"cpu\").detach() for y,l in zip(y_prob3,lang)]\n",
    "    probs = torch.tensor(np.array([prob01,prob02,prob03])).transpose(0,1)\n",
    "    #print(probs)\n",
    "    loss = criterion(probs.to(device),label)\n",
    "    losss.append(loss.item())\n",
    "    pred_labels = torch.argmax(probs,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])\n",
    "    acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "    accs.append(acc.item())\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        id = 0 #np.random.randint(len(cols))\n",
    "        imgs = cols[0].to(\"cpu\")\n",
    "        c_langs = [i2w[idx] for idx in lang[id].to(\"cpu\").tolist()]\n",
    "        g_langs = [i2w[idx] for idx in output_lang[id].to(\"cpu\").tolist()] \\\n",
    "                + [\"|\"]+ [i2w[idx] for idx in output_lang2[id].to(\"cpu\").tolist()] \\\n",
    "                + [\"|\"]+ [i2w[idx] for idx in output_lang3[id].to(\"cpu\").tolist()]\n",
    "        label = label[id]\n",
    "        prob = probs[id]\n",
    "        print(prob.to(\"cpu\").tolist())\n",
    "        check_data(imgs, prob, g_langs, c_langs, cap=\"L1 prediction\")\n",
    "        print(torch.exp(probs[id]))\n",
    "        print(torch.argmax(probs[id]))\n",
    "        #print(\"Loss: \",loss.item())\n",
    "\n",
    "print(\"Accuracy:,\",np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
