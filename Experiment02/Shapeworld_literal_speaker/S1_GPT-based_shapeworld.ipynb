{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from shapeworld_data import load_raw_data, get_vocab, ShapeWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_raw_data(imgs, labels, langs, id=0):\n",
    "    data = list(zip(imgs,labels,langs))\n",
    "    img_list,label,lang = data[id]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 2))\n",
    "    fig.suptitle(\" \".join(lang))\n",
    "    for i,(l,img) in enumerate(zip(label,img_list)):\n",
    "        img = img.transpose((2,1,0))\n",
    "        axes[i].imshow(img)\n",
    "        if l==1: axes[i].set_title(\"Correct\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    tokenized = tokenizer.encode(sentence)\n",
    "    #print(tokenized)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.path.abspath('')).parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\\shapeworld_np\")\n",
    "print(data_path)\n",
    "data_list = os.listdir(data_path)\n",
    "print(data_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab([os.path.join(data_path,d) for d in data_list])\n",
    "print(vocab[\"w2i\"])\n",
    "\n",
    "COLOR = {\"white\":[1,0,0,0,0,0], \"green\":[0,1,0,0,0,0], \"gray\":[0,0,1,0,0,0], \"yellow\":[0,0,0,1,0,0], \"red\":[0,0,0,0,1,0], \"blue\":[0,0,0,0,0,1], \"other\":[0,0,0,0,0,0]}\n",
    "SHAPE = {\"shape\":[0,0,0,0], \"square\":[1,0,0,0], \"circle\":[0,1,0,0], \"rectangle\":[0,0,1,0], \"ellipse\":[0,0,0,1]}\n",
    "\n",
    "print(\"Generating Vocab_dict from GPT tokenizer ...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "gpt_vocab_dict = tokenizer.get_vocab()\n",
    "print(\"Length of the GPT Vocab list is \",len(gpt_vocab_dict.keys()))\n",
    "\n",
    "PAD = 15636\n",
    "SOS= EOS = UNK = 50256\n",
    "original_PAD = 0\n",
    "original_SOS = 1\n",
    "original_EOS = 2\n",
    "original_UNK = 3\n",
    "\n",
    "w2i = vocab[\"w2i\"]\n",
    "i2w = vocab[\"i2w\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepapre the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[0]))\n",
    "imgs = d[\"imgs\"]\n",
    "labels = d[\"labels\"]\n",
    "langs = d[\"langs\"]\n",
    "check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"])\n",
    "for i in range(1,5):\n",
    "    d = load_raw_data(os.path.join(data_path,data_list[i]))\n",
    "    imgs = np.vstack((imgs,d[\"imgs\"]))\n",
    "    labels = np.vstack((labels,d[\"labels\"]))\n",
    "    langs = np.hstack((langs,d[\"langs\"]))\n",
    "    #check_raw_data(d[\"imgs\"],d[\"labels\"],d[\"langs\"])\n",
    "\n",
    "imgs_data_tensor = torch.tensor(imgs,dtype=torch.float)\n",
    "label_data_tensor = torch.tensor(labels)\n",
    "context_id_data = list(map(sentence2index,langs))\n",
    "max_context_len = np.max([len(c) for c in context_id_data])\n",
    "padded_context_data = torch.tensor(np.array([[SOS]+c+[EOS]+[PAD]*(max_context_len-len(c)) for c in context_id_data]))   # <sos>+context+<eos>+<pad>*\n",
    "print(imgs_data_tensor.shape, label_data_tensor.shape, padded_context_data.shape)\n",
    "\n",
    "data = [(img,u,l) for img,l,u in zip(imgs_data_tensor,label_data_tensor,padded_context_data)]\n",
    "test_split = 1000\n",
    "train_data, test_data = data[:-test_split], data[-test_split:]\n",
    "print(\"Train, Test data length = \",len(train_data),\",\",len(test_data))\n",
    "\n",
    "train_batch = DataLoader(dataset=train_data,batch_size=32,shuffle=True,num_workers=0)\n",
    "test_batch = DataLoader(dataset=test_data,batch_size=32,shuffle=False,num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import CNN_encoder\n",
    "\n",
    "class Imgs_emb_DeepSet(nn.Module):\n",
    "    def __init__(self, input_size=10, output_size=20):\n",
    "        super(Imgs_emb_DeepSet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, output_size)\n",
    "        self.linear2 = nn.Linear(input_size, output_size)\n",
    "        self.linear3 = nn.Linear(output_size, output_size)\n",
    "    \n",
    "    def forward(self, img_emb1, img_emb2):\n",
    "        img_embs = F.relu(self.linear1(img_emb1)) + F.relu(self.linear2(img_emb2))\n",
    "        img_embs = self.linear3(img_embs)\n",
    "        return img_embs\n",
    "\n",
    "\n",
    "class CS_CNN_Encoder(nn.Module):\n",
    "    def __init__(self, input_size=10, output_size=10):\n",
    "        super(CS_CNN_Encoder, self).__init__()\n",
    "        self.cnn_color_encoder = CNN_encoder(6)\n",
    "        self.cnn_color_encoder.load_state_dict(torch.load(\"model_params/cnn_color_model_best-loss.pth\",map_location=device))\n",
    "        for params in self.cnn_color_encoder.parameters(): params.requires_grad = False\n",
    "        self.cnn_shape_encoder = CNN_encoder(4)\n",
    "        self.cnn_shape_encoder.load_state_dict(torch.load(\"model_params/cnn_shape_model_best-loss.pth\",map_location=device))\n",
    "        for params in self.cnn_shape_encoder.parameters(): params.requires_grad = False\n",
    "        self.deepset_size = 6+4\n",
    "        self.deepset = Imgs_emb_DeepSet(self.deepset_size, self.deepset_size)\n",
    "        self.linear = nn.Linear(input_size+self.deepset_size, output_size)\n",
    "\n",
    "    def get_feat_emb(self,feat):\n",
    "        col_embs = self.cnn_color_encoder(feat)\n",
    "        shape_embs = self.cnn_shape_encoder(feat)\n",
    "        img_embs = torch.hstack((col_embs,shape_embs))\n",
    "        return img_embs\n",
    "    \n",
    "    def forward(self,feats,labels):\n",
    "        idxs = [0,1,2]\n",
    "        target_idx = int(torch.argmax(labels))\n",
    "        idxs.remove(target_idx)\n",
    "        other_idx1,other_idx2 = idxs[0],idxs[1]\n",
    "        target_img,other_img1,other_img2 = feats[:,target_idx], feats[:,other_idx1], feats[:,other_idx2]\n",
    "        target_embs = self.get_feat_emb(target_img)\n",
    "        other_embs1 = self.get_feat_emb(other_img1)\n",
    "        other_embs2 = self.get_feat_emb(other_img2)                 # (batch_size,10)\n",
    "        other_embs = F.relu(self.deepset(other_embs1,other_embs2))  # (batch_size,10)\n",
    "        embs = torch.hstack((target_embs,other_embs))               # (batch_size,20)\n",
    "        feat = self.linear(embs)                                    # (batch_size,output_size)\n",
    "        return feat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder-decoder model\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "class S1_EncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=768):\n",
    "        super(S1_EncoderDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = CS_CNN_Encoder(input_size, hidden_size)\n",
    "        self.decoder = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"distilgpt2\").decoder\n",
    "\n",
    "    def forward(self, feats, labels, langs):\n",
    "        batch_size = len(feats)\n",
    "        encoder_hidden = self.encoder(feats, labels)\n",
    "        decoder_hidden = encoder_hidden.reshape(batch_size,1,self.hidden_size)\n",
    "        decoder_input = langs[:,:-1]\n",
    "        decoder_output = self.decoder(input_ids=decoder_input, encoder_hidden_states=decoder_hidden)\n",
    "        return decoder_output[0]\n",
    "    \n",
    "    def generate(self,feats,labels,max_len=5,temperature=0.7):\n",
    "        batch_size = len(feats)\n",
    "        encoder_hidden = self.encoder(feats, labels)\n",
    "        decoder_hidden = encoder_hidden.reshape(batch_size,1,self.hidden_size)\n",
    "        sos = \"<|endoftext|>\"\n",
    "        generated = torch.tensor(tokenizer.encode(sos)*batch_size).unsqueeze(1).to(decoder_hidden.device)\n",
    "        probs_list = torch.zeros(batch_size,50257)\n",
    "        probs_list[:,SOS] = 1.0\n",
    "        probs_list = probs_list.unsqueeze(1).to(decoder_hidden.device)\n",
    "        for i in range(max_len):\n",
    "            #print(generated.shape)\n",
    "            decoder_output = self.decoder(input_ids=generated, encoder_hidden_states=decoder_hidden)\n",
    "            logits = decoder_output[0][:,-1,:]/temperature\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            probs_list = torch.cat((probs_list,probs.unsqueeze(1)),dim=1)\n",
    "        return generated,probs_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def check_data(img_list,c_lang,g_lang,lis_label,cscnn=None,gg_lang=None):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 4))\n",
    "    context = \"Correct: \"+\" \".join(c_lang)\n",
    "    if cscnn is not None: context += \"\\nCSCNN:\"+cscnn\n",
    "    context += \"\\nGenerated:\"+\" \".join(g_lang)\n",
    "    if gg_lang is not None: context += \"\\n Generated02:\"+re.sub(r\"<\\|endoftext\\|>|pad\",\" \",gg_lang)\n",
    "    fig.suptitle(context)\n",
    "    for i,(l,img) in enumerate(zip(label,img_list)):\n",
    "        img = img.transpose(2,0).to(\"cpu\").detach().numpy()\n",
    "        axes[i].imshow(img)\n",
    "        if torch.argmax(lis_label)==i:axes[i].set_title(\"Listener Prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0, CNN_encoder\n",
    "\n",
    "feat_dim = 10\n",
    "speaker = S1_EncoderDecoder(input_size=feat_dim)\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "cnn_color_encoder = CNN_encoder(6).to(device)\n",
    "cnn_color_encoder.load_state_dict(torch.load(\"model_params/cnn_color_model_best-loss.pth\",map_location=device))\n",
    "for params in cnn_color_encoder.parameters(): params.requires_grad = False\n",
    "cnn_shape_encoder = CNN_encoder(4).to(device)\n",
    "cnn_shape_encoder.load_state_dict(torch.load(\"model_params/cnn_shape_model_best-loss.pth\",map_location=device))\n",
    "for params in cnn_shape_encoder.parameters(): params.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(speaker.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "\n",
    "max_len = 5\n",
    "\n",
    "epoch = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "best_loss = 100\n",
    "for i in range(epoch):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    speaker.train()\n",
    "    for cols,lang,label in tqdm(train_batch):\n",
    "        cols, lang, label = cols.to(device), lang.type(torch.LongTensor).to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = speaker(cols, label, lang)\n",
    "        # Compute the loss\n",
    "        output_view = output.view(-1, output.shape[-1])\n",
    "        target = lang[:,1:].reshape(-1)\n",
    "        lang_loss = criterion(output_view, target)\n",
    "        lang_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += lang_loss.item()\n",
    "        #break\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for cols,lang,label in tqdm(test_batch):\n",
    "            cols, lang, label = cols.to(device), lang.type(torch.LongTensor).to(device), label.to(device)\n",
    "            output = speaker(cols, label, lang)\n",
    "            target_col, target_shape = cnn_color_encoder(cols[:,0]), cnn_shape_encoder(cols[:,0])\n",
    "            target_cols = [list(COLOR.keys())[int(torch.argmax(col_vec))] for col_vec in target_col]\n",
    "            target_shapes = [list(SHAPE.keys())[int(torch.argmax(shape_vec))] for shape_vec in target_shape]\n",
    "            direct_target_utter = [c+\" \"+s for c,s in zip(target_cols,target_shapes)]\n",
    "            output_view = output.view(-1, output.shape[-1])\n",
    "            target = lang[:,1:].reshape(-1)\n",
    "            lang_loss = criterion(output_view, target)\n",
    "            test_loss += lang_loss.item()\n",
    "            generated,lang_probs = speaker.generate(cols,label,max_len=max_len)\n",
    "            #break\n",
    "        batch_test_loss = test_loss/len(test_batch)\n",
    "\n",
    "    print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    test_loss_list.append(batch_test_loss)\n",
    "    if batch_test_loss < best_loss:\n",
    "        print(\"Best loss saved ...\")\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/shapeworld_S1-GPT2-decoder_loss=Lang_best-loss_100epoch.pth\")\n",
    "        best_loss = batch_test_loss\n",
    "    if i%10 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        cols = cols[id].to(\"cpu\")\n",
    "        c_langs = tokenizer.decode([idx for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        g_langs = tokenizer.decode([idx for idx in output.argmax(2)[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        lis_label = torch.zeros(3)\n",
    "        lis_label[0] = 1.0\n",
    "        check_data(cols, c_langs, g_langs, lis_label, cscnn=direct_target_utter[id],gg_lang=tokenizer.decode(generated[id]))\n",
    "    #break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0, CNN_encoder\n",
    "\n",
    "feat_dim = 10\n",
    "speaker = S1_EncoderDecoder(input_size=feat_dim).to(device)\n",
    "best = \"model_params/shapeworld_S1-GPT2-decoder_loss=Lang_best-loss_100epoch.pth\"\n",
    "speaker.load_state_dict(torch.load(best,map_location=device))\n",
    "speaker.to(device)\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "print(\"Model loaded ...\")\n",
    "\n",
    "optimizer = optim.Adam(speaker.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "max_len = 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(COLOR.keys())\n",
    "col_list[-1] = \"\"\n",
    "shape_list = list(SHAPE.keys())\n",
    "utter_list = [\" \".join([w for w in (c+\" \"+s).split(\" \") if w]) for c in col_list for s in shape_list+[\"\"]]\n",
    "gpt_utter_list = [\"\".join([w for w in (c+\" \"+s).split(\" \") if w]) for c in col_list for s in shape_list+[\"\"]]\n",
    "vocab2gpt = {g:u for u,g in zip(utter_list,gpt_utter_list)}\n",
    "for g,u in vocab2gpt.items():\n",
    "    print(u,\" : \",g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "\n",
    "def decode_gpt_vocab(w):\n",
    "    if w in w2i.keys(): \n",
    "        return [w2i[w]]\n",
    "    elif w in vocab2gpt.keys():\n",
    "        return [w2i[t] for t in vocab2gpt[w].split(\" \")]\n",
    "    else:\n",
    "        return [original_UNK]\n",
    "\n",
    "def gpt_lang2L0_lang(generated_langs):\n",
    "    langs = [tokenizer.decode([idx for idx in generated if idx not in [PAD,SOS,EOS]]) for generated in generated_langs]\n",
    "    tokens = []\n",
    "    for l in langs:\n",
    "        decoded = [decode_gpt_vocab(w) for w in word_tokenize(l)]+[[],[]]\n",
    "        tokens.append(list(reduce(lambda x,y:x+y,decoded)))\n",
    "    max_tokens_len = max([len(t) for t in tokens])\n",
    "    padded_tokens = torch.tensor(np.array([[original_SOS]+ts+[original_EOS]+[original_PAD]*(max_tokens_len-len(ts)) for ts in tokens]))\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "speaker.eval()\n",
    "#speaker = speaker.cpu()\n",
    "with torch.no_grad():\n",
    "    for i,(cols,lang,label) in enumerate(test_batch):\n",
    "        cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "        generated_lang, lang_probs = speaker.generate(cols,label,max_len=max_len)\n",
    "        output_lang = gpt_lang2L0_lang(generated_lang).to(device)\n",
    "        literal_listener.eval()\n",
    "        #literal_listener = literal_listener.cpu()\n",
    "        lis_labels = literal_listener(cols, output_lang)\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(cols.shape[0])\n",
    "        acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "        accs.append(acc.item())\n",
    "        if i%10 == 0:\n",
    "            print(i+1,\"/\",len(test_batch))\n",
    "            id = np.random.randint(len(cols))\n",
    "            cols = cols[id].to(\"cpu\")\n",
    "            print([i2w[int(idx)] for idx in output_lang[id]])\n",
    "            c_langs = tokenizer.decode([idx for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "            g_langs = tokenizer.decode([idx for idx in generated_lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "            label = label[id]\n",
    "            #print(int(torch.argmax(lis_labels[id])))\n",
    "            check_data(cols, c_langs, g_langs, lis_labels[id])\n",
    "\n",
    "print(\"Accuracy: \",np.mean(accs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss = []\n",
    "accs = []\n",
    "speaker.eval()\n",
    "\n",
    "for i,(cols,lang,label) in enumerate(test_batch):\n",
    "    cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "    # for 1st image\n",
    "    label01 = torch.zeros_like(label)\n",
    "    label01[:,0] = 1.0\n",
    "    generated_lang1, lang_probs1 = speaker.generate(cols,label01,max_len=max_len)\n",
    "    # for 2nd image\n",
    "    label02 = torch.zeros_like(label)\n",
    "    label02[:,1] = 1.0\n",
    "    generated_lang2, lang_probs2 = speaker.generate(cols,label02,max_len=max_len)\n",
    "    # for 3rd image\n",
    "    label03 = torch.zeros_like(label)\n",
    "    label03[:,2] = 1.0\n",
    "    generated_lang3, lang_probs3 = speaker.generate(cols,label03,max_len=max_len)\n",
    "    \n",
    "    # compute the probability\n",
    "    prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_probs1,lang))]\n",
    "    prob01_sums = list(map(sum,prob01))\n",
    "    prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_probs2,lang))]\n",
    "    prob02_sums = list(map(sum,prob02))\n",
    "    prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_probs3,lang))]\n",
    "    prob03_sums = list(map(sum,prob03))\n",
    "    probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "    #print(probs)\n",
    "    loss = criterion(probs.to(device),label)\n",
    "    losss.append(loss.item())\n",
    "    pred_labels = torch.argmax(probs,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])\n",
    "    acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "    accs.append(acc.item())\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        imgs = cols[id].to(\"cpu\")\n",
    "        c_langs = tokenizer.decode([idx for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        g_langs = tokenizer.decode([idx for idx in generated_lang1[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\\\n",
    "                + [\"|\"]+ tokenizer.decode([idx for idx in generated_lang2[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \") \\\n",
    "                + [\"|\"]+ tokenizer.decode([idx for idx in generated_lang3[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        label = label[id]\n",
    "        check_data(imgs, c_langs, g_langs, probs[id])\n",
    "        print(torch.exp(probs[id]))\n",
    "        print(torch.where(torch.exp(probs[id])>0.1,1,0))\n",
    "\n",
    "print(\"Loss: \",np.mean(losss))\n",
    "print(\"Accuracy: \",np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
