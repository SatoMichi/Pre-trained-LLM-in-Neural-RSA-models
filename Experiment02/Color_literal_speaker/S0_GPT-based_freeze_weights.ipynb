{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from corpus import ColorsCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_data(cols, contexts, id=0):\n",
    "    col01,col02,col03 = cols[id]\n",
    "    context = contexts[id]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 3))\n",
    "    fig.suptitle(context)\n",
    "    # plot correct color, col01\n",
    "    ec = col01\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col01, ec=ec, lw=8)\n",
    "    axes[0].add_patch(patch)\n",
    "    axes[0].axis('off')\n",
    "    #axes[0].set_title(str(col01))\n",
    "    # plot wrong color, col02\n",
    "    ec = col02\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col02, ec=ec, lw=8)\n",
    "    axes[1].add_patch(patch)\n",
    "    axes[1].axis('off')\n",
    "    #axes[1].set_title(str(col02))\n",
    "    # plot wrong color, col02\n",
    "    ec = \"black\"\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col03, ec=ec, lw=8)\n",
    "    axes[2].add_patch(patch)\n",
    "    axes[2].axis('off')\n",
    "    #axes[2].set_title(str(col03))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def sentence2index(sentence):\n",
    "    tokenized = tokenizer.encode(sentence)\n",
    "    #print(tokenized)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare raw data\n",
    "root = Path(os.path.abspath('')).parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\")\n",
    "print(data_path)\n",
    "corpus = ColorsCorpusReader(os.path.join(data_path,\"colors.csv\"), word_count=None, normalize_colors=True)\n",
    "examples = list(corpus.read())\n",
    "print(\"Number of datapoints: {}\".format(len(examples)))\n",
    "# balance positive and negative samples\n",
    "colors_data = [e.get_context_data()[0] for e in examples]\n",
    "utterance_data = [e.get_context_data()[1] for e in examples]\n",
    "check_row_data(colors_data,utterance_data,id=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# generate vocab dict\n",
    "if not os.path.exists(\"vocab.pkl\"):\n",
    "    print(\"Generating vocab dict ...\")\n",
    "    vocab_list = list(set(reduce(lambda x,y:x+y,[word_tokenize(c) for c in utterance_data]))) # with nltk.tokenizer, 3953 vocabs\n",
    "    vocab_list = [\"<pad>\",\"<sos>\",\"<eos>\",\"<unk>\"] + vocab_list                               # Added padding for batching\n",
    "    vocab_dict = dict(zip(vocab_list,list(range(len(vocab_list)))))\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab_dict, f)\n",
    "elif not GPT:\n",
    "    print(\"Loading vocab dict ...\")\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab_dict = pickle.load(f)\n",
    "else:\n",
    "    print(\"Generating Vocab_dict from GPT tokenizer ...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "    gpt_vocab_dict = tokenizer.get_vocab()\n",
    "    print(\"Loading vocab dict ...\")\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab_dict = pickle.load(f)\n",
    "\n",
    "print(\"Length of the Vocab list is \",len(gpt_vocab_dict.keys()))\n",
    "if not GPT:\n",
    "    print(\"<pad> id = \",vocab_dict[\"<pad>\"])\n",
    "    print(\"<sos> id = \",vocab_dict[\"<sos>\"])\n",
    "    print(\"<eos> id = \",vocab_dict[\"<eos>\"])\n",
    "    print(\"<unk> id = \",vocab_dict[\"<unk>\"])\n",
    "    print(\"blue id = \",vocab_dict[\"blue\"])\n",
    "    print(\"red id = \",vocab_dict[\"red\"])\n",
    "    print(\"green id = \",vocab_dict[\"green\"])\n",
    "    PAD = 0\n",
    "    SOS = 1\n",
    "    EOS = 2\n",
    "    UNK = 3\n",
    "else:\n",
    "    print(\"PAD id = \",gpt_vocab_dict[\"pad\"])\n",
    "    print(\"BOS id = \",gpt_vocab_dict[\"<|endoftext|>\"])\n",
    "    print(\"EOS id = \",gpt_vocab_dict[\"<|endoftext|>\"])\n",
    "    print(\"UNK id = \",gpt_vocab_dict[\"<|endoftext|>\"])\n",
    "    print(\"blue id = \",gpt_vocab_dict[\"blue\"])\n",
    "    print(\"red id = \",gpt_vocab_dict[\"red\"])\n",
    "    print(\"green id = \",gpt_vocab_dict[\"green\"])\n",
    "    PAD = 15636\n",
    "    SOS= EOS = UNK = 50256\n",
    "    original_PAD = 0\n",
    "    original_SOS = 1\n",
    "    original_EOS = 2\n",
    "    original_UNK = 3\n",
    "\n",
    "w2i = vocab_dict\n",
    "i2w = {k:v for (v,k) in vocab_dict.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepapre the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_data_tensor = torch.tensor(np.array(colors_data),dtype=torch.float)\n",
    "context_id_data = list(map(sentence2index,utterance_data))\n",
    "max_context_len = np.max([len(c) for c in context_id_data])\n",
    "print(\"MAX length = \",max_context_len)\n",
    "padded_context_data = torch.tensor(np.array([[SOS]+c+[EOS]+[PAD]*(max_context_len-len(c)) for c in context_id_data]))   # <sos>+context+<eos>+<pad>*\n",
    "print(\"Colors shape = \",colors_data_tensor.shape)\n",
    "print(\"Padded context id lists shape = \",padded_context_data.shape)\n",
    "\n",
    "data = [(color,torch.tensor(context,dtype=torch.long)) for color,context in zip(colors_data_tensor,padded_context_data)]\n",
    "label = torch.zeros(len(data),3)\n",
    "label[:,2] = 1.0\n",
    "print(\"total data length = \",len(data))\n",
    "print(\"total label shape = \",label.shape)\n",
    "\n",
    "test_split = 7000\n",
    "train_data, test_data = data[:-test_split], data[-test_split:]\n",
    "train_label, test_label = label[:-test_split], label[-test_split:]\n",
    "print(\"Train, Test data length = \",len(train_data),\",\",len(test_data))\n",
    "print(\"Train, Test label length = \",len(train_label),\",\",len(test_label))\n",
    "\n",
    "train_dataset = list(zip(train_data,train_label))\n",
    "test_dataset = list(zip(test_data,test_label))\n",
    "train_batch = DataLoader(dataset=train_dataset,batch_size=32,shuffle=True,num_workers=0)\n",
    "test_batch = DataLoader(dataset=test_dataset,batch_size=32,shuffle=False,num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For permutation invariance\n",
    "class Colors_DeepSet(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=16):\n",
    "        super(Colors_DeepSet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, output_size)\n",
    "        self.linear2 = nn.Linear(output_size, output_size)\n",
    "    \n",
    "    def forward(self, col1, col2):\n",
    "        col_emb1 = F.relu(self.linear1(col1))\n",
    "        col_emb2 = F.relu(self.linear1(col2))\n",
    "        col_embs = col_emb1 + col_emb2\n",
    "        col_embs = self.linear2(col_embs)\n",
    "        return col_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors_Feature_Encoder(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=16):\n",
    "        super(Colors_Feature_Encoder, self).__init__()\n",
    "        self.deepset_size = 16\n",
    "        self.deepset = Colors_DeepSet(input_size, self.deepset_size)\n",
    "        self.linear = nn.Linear(input_size+self.deepset_size, hidden_size)\n",
    "\n",
    "    def forward(self,feats,labels):\n",
    "        idxs = [0,1,2]\n",
    "        target_idx = int(torch.argmax(labels))\n",
    "        idxs.remove(target_idx)\n",
    "        other_idx1,other_idx2 = idxs[0],idxs[1]\n",
    "        target_col,col1,col2 = feats[:,target_idx], feats[:,other_idx1], feats[:,other_idx2]\n",
    "        col_embs = F.relu(self.deepset(col1,col2))\n",
    "        cols = torch.hstack((target_col,col_embs))\n",
    "        feat = self.linear(cols)\n",
    "        return feat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Encoder-Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder-decoder model\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "class S1_EncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=768):\n",
    "        super(S1_EncoderDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = Colors_Feature_Encoder(input_size, hidden_size)\n",
    "        self.decoder = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"distilgpt2\").decoder\n",
    "\n",
    "    def forward(self, feats, labels, langs):\n",
    "        batch_size = len(feats)\n",
    "        encoder_hidden = self.encoder(feats, labels)\n",
    "        decoder_hidden = encoder_hidden.reshape(batch_size,1,self.hidden_size)\n",
    "        decoder_input = langs[:,:-1]\n",
    "        decoder_output = self.decoder(input_ids=decoder_input, encoder_hidden_states=decoder_hidden)\n",
    "        return decoder_output[0]\n",
    "    \n",
    "    def generate(self,feats,labels,max_len=5,temperature=0.7):\n",
    "        batch_size = len(feats)\n",
    "        encoder_hidden = self.encoder(feats, labels)\n",
    "        decoder_hidden = encoder_hidden.reshape(batch_size,1,self.hidden_size)\n",
    "        sos = \"<|endoftext|>\"\n",
    "        generated = torch.tensor(tokenizer.encode(sos)*batch_size).unsqueeze(1).to(decoder_hidden.device)\n",
    "        probs_list = torch.zeros(batch_size,50257)\n",
    "        probs_list[:,SOS] = 1.0\n",
    "        probs_list = probs_list.unsqueeze(1).to(decoder_hidden.device)\n",
    "        for i in range(max_len):\n",
    "            #print(generated.shape)\n",
    "            decoder_output = self.decoder(input_ids=generated, encoder_hidden_states=decoder_hidden)\n",
    "            logits = decoder_output[0][:,-1,:]/temperature\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            probs_list = torch.cat((probs_list,probs.unsqueeze(1)),dim=1)\n",
    "        return generated,probs_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_color import Emb_RNN_L0\n",
    "\n",
    "speaker = S1_EncoderDecoder(input_size=3)\n",
    "speaker.to(device)\n",
    "\n",
    "# First freeze all the weights\n",
    "base_model = speaker.decoder.get_output_embeddings()\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "# The release the final one layers out of 6 layers\n",
    "last_two_layers = speaker.decoder.transformer.h[-1:]\n",
    "for layer in last_two_layers:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "literal_listener = Emb_RNN_L0(len(vocab_dict)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\emb-rnn-l0_epoch=100_best-acc.pth\",map_location=device))\n",
    "\n",
    "optimizer = optim.Adam(speaker.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
    "\n",
    "max_len = 5\n",
    "\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(cols, label, c_lang, g_lang, lis_label):\n",
    "    col01,col02,col03 = cols[0].numpy(), cols[1].numpy(), cols[2].numpy()\n",
    "    context = \"Correct: \"+\" \".join(c_lang)+\"\\nGenerated:\"+\" \".join(g_lang)+\"\\n \"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 6))\n",
    "    fig.suptitle(context)\n",
    "    # plot correct color, col01\n",
    "    ec = \"black\" if label[0] > 0.5 else col01\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col01, ec=ec, lw=8)\n",
    "    axes[0].add_patch(patch)\n",
    "    axes[0].axis('off')\n",
    "    if torch.argmax(lis_label)==0:axes[0].set_title(\"Listener Prediction\")\n",
    "    # plot wrong color, col02\n",
    "    ec = \"black\" if label[1] > 0.5 else col02\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col02, ec=ec, lw=8)\n",
    "    axes[1].add_patch(patch)\n",
    "    axes[1].axis('off')\n",
    "    if torch.argmax(lis_label)==1:axes[1].set_title(\"Listener Prediction\")\n",
    "    # plot wrong color, col02\n",
    "    ec = \"black\" if label[2] > 0.5 else col03\n",
    "    patch = mpatch.Rectangle((0, 0), 1, 1, color=col03, ec=ec, lw=8)\n",
    "    axes[2].add_patch(patch)\n",
    "    axes[2].axis('off')\n",
    "    if torch.argmax(lis_label)==2:axes[2].set_title(\"Listener Prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "test_l0_acc_list = []\n",
    "best_loss = 100\n",
    "for i in range(epoch):\n",
    "    print(\"##############################################\")\n",
    "    print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    literal_listener.eval()\n",
    "    speaker.train()\n",
    "    for (cols,lang),label in tqdm(train_batch):\n",
    "        cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = speaker(cols, label, lang)\n",
    "        # Compute the loss\n",
    "        output_view = output.view(-1, output.shape[-1])\n",
    "        target = lang[:,1:].reshape(-1)\n",
    "        lang_loss = criterion(output_view, target)\n",
    "        lang_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += lang_loss.item()\n",
    "        #break\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for (cols,lang),label in tqdm(test_batch):\n",
    "            cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "            output = speaker(cols, label, lang)\n",
    "            output_view = output.view(-1, output.shape[-1])\n",
    "            target = lang[:,1:].reshape(-1)\n",
    "            lang_loss = criterion(output_view, target)\n",
    "            test_loss += lang_loss.item()\n",
    "            generated,lang_probs = speaker.generate(cols,label,max_len=max_len)\n",
    "            #break\n",
    "        batch_test_loss = test_loss/len(test_batch)\n",
    "\n",
    "    print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "    train_loss_list.append(batch_train_loss)\n",
    "    test_loss_list.append(batch_test_loss)\n",
    "    if batch_test_loss < best_loss:\n",
    "        print(\"Best loss saved ...\")\n",
    "        torch.save(speaker.to(device).state_dict(),\"model_params/color_S1-GPT2-decoder_loss=Lang_best-loss_freeze-1.pth\")\n",
    "        best_loss = batch_test_loss\n",
    "    if i%1 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        cols = cols[id].to(\"cpu\")\n",
    "        c_langs = tokenizer.decode([idx for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        g_langs = tokenizer.decode([idx for idx in output.argmax(2)[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        lis_label = torch.zeros(3)\n",
    "        lis_label[2] = 1.0\n",
    "        check_data(cols, label[id], c_langs, g_langs, lis_label)\n",
    "        print(tokenizer.decode(generated[id]))\n",
    "    #break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_color import Emb_RNN_L0\n",
    "from tqdm import tqdm\n",
    "\n",
    "speaker = S1_EncoderDecoder(input_size=3).to(device)\n",
    "best = \"model_params/color_S1-GPT2-decoder_loss=Lang_best-loss_freeze-1.pth\"\n",
    "speaker.load_state_dict(torch.load(best,map_location=device))\n",
    "speaker.to(device)\n",
    "print(\"Model loaded ...\")\n",
    "\n",
    "literal_listener = Emb_RNN_L0(len(vocab_dict)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\emb-rnn-l0_epoch=100_best-acc.pth\",map_location=device))\n",
    "\n",
    "optimizer = optim.Adam(speaker.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "max_len = 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def gpt_lang2L0_lang(generated_langs):\n",
    "    langs = [tokenizer.decode([idx for idx in generated if idx not in [PAD,SOS,EOS]]) for generated in generated_langs]\n",
    "    tokens = [[w2i.get(w,original_UNK) for w in word_tokenize(l)] for l in langs]\n",
    "    max_tokens_len = max([len(t) for t in tokens])\n",
    "    padded_tokens = torch.tensor(np.array([[original_SOS]+ts+[original_EOS]+[original_PAD]*(max_tokens_len-len(ts)) for ts in tokens]))\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "speaker.eval()\n",
    "#speaker = speaker.cpu()\n",
    "with torch.no_grad():\n",
    "    for i,((cols,lang),label) in enumerate(test_batch):\n",
    "        cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "        generated_lang, lang_probs = speaker.generate(cols,label,max_len=max_len)\n",
    "        output_lang = gpt_lang2L0_lang(generated_lang).to(device)\n",
    "        literal_listener.eval()\n",
    "        #literal_listener = literal_listener.cpu()\n",
    "        lis_labels = literal_listener(cols, output_lang)\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(cols.shape[0])+2\n",
    "        acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "        accs.append(acc.item())\n",
    "        # if i%100 == 0:\n",
    "        #     print(i+1,\"/\",len(test_batch))\n",
    "        #     id = np.random.randint(len(cols))\n",
    "        #     cols = cols[id].to(\"cpu\")\n",
    "        #     c_langs = tokenizer.decode([idx for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        #     g_langs = tokenizer.decode([idx for idx in generated_lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        #     label = label[id]\n",
    "        #     #print(int(torch.argmax(lis_labels[id])))\n",
    "        #     check_data(cols, label, c_langs, g_langs, lis_labels[id])\n",
    "\n",
    "print(\"Accuracy: \",np.mean(accs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losss = []\n",
    "accs = []\n",
    "speaker.eval()\n",
    "\n",
    "for i,((cols,lang),label) in enumerate(test_batch):\n",
    "    cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "    # for 1st image\n",
    "    label01 = torch.zeros_like(label)\n",
    "    label01[:,0] = 1.0\n",
    "    generated_lang1, lang_probs1 = speaker.generate(cols,label01,max_len=max_len)\n",
    "    # for 2nd image\n",
    "    label02 = torch.zeros_like(label)\n",
    "    label02[:,1] = 1.0\n",
    "    generated_lang2, lang_probs2 = speaker.generate(cols,label02,max_len=max_len)\n",
    "    # for 3rd image\n",
    "    label03 = torch.zeros_like(label)\n",
    "    label03[:,2] = 1.0\n",
    "    generated_lang3, lang_probs3 = speaker.generate(cols,label03,max_len=max_len)\n",
    "    \n",
    "    # compute the probability\n",
    "    prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_probs1,lang))]\n",
    "    prob01_sums = list(map(sum,prob01))\n",
    "    prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_probs2,lang))]\n",
    "    prob02_sums = list(map(sum,prob02))\n",
    "    prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "              for batch,(sent,idxs) in enumerate(zip(lang_probs3,lang))]\n",
    "    prob03_sums = list(map(sum,prob03))\n",
    "    probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "    #print(probs)\n",
    "    loss = criterion(probs.to(device),label)\n",
    "    losss.append(loss.item())\n",
    "    pred_labels = torch.argmax(probs,dim=1)\n",
    "    correct_labels = torch.zeros(cols.shape[0])+2\n",
    "    acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "    accs.append(acc.item())\n",
    "    \n",
    "    if i%40 == 0:\n",
    "        id = np.random.randint(len(cols))\n",
    "        imgs = cols[id].to(\"cpu\")\n",
    "        c_langs = tokenizer.decode([idx for idx in lang[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        g_langs = tokenizer.decode([idx for idx in generated_lang1[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\\\n",
    "                + [\"|\"]+ tokenizer.decode([idx for idx in generated_lang2[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \") \\\n",
    "                + [\"|\"]+ tokenizer.decode([idx for idx in generated_lang3[id].to(\"cpu\").tolist() if idx not in [PAD,SOS,EOS]]).split(\" \")\n",
    "        label = label[id]\n",
    "        check_data(imgs, label, c_langs, g_langs, probs[id])\n",
    "        print(torch.exp(probs[id]))\n",
    "        print(torch.where(torch.exp(probs[id])>0.1,1,0))\n",
    "\n",
    "print(\"Loss: \",np.mean(losss))\n",
    "print(\"Accuracy: \",np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
