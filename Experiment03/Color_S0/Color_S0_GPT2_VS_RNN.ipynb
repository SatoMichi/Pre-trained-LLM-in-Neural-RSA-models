{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from corpus import ColorsCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device = \",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pickle\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "print(\"Generating Vocab_dict from GPT tokenizer ...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "gpt_vocab_dict = tokenizer.get_vocab()\n",
    "print(\"Loading vocab dict ...\")\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab_dict = pickle.load(f)\n",
    "\n",
    "print(\"Length of the Vocab list is \",len(gpt_vocab_dict.keys()))\n",
    "print(\"PAD id = \",gpt_vocab_dict[\"pad\"])\n",
    "print(\"BOS id = \",gpt_vocab_dict[\"<|endoftext|>\"])\n",
    "print(\"EOS id = \",gpt_vocab_dict[\"<|endoftext|>\"])\n",
    "print(\"UNK id = \",gpt_vocab_dict[\"<|endoftext|>\"])\n",
    "print(\"blue id = \",gpt_vocab_dict[\"blue\"])\n",
    "print(\"red id = \",gpt_vocab_dict[\"red\"])\n",
    "print(\"green id = \",gpt_vocab_dict[\"green\"])\n",
    "PAD = 15636\n",
    "SOS= EOS = UNK = 50256\n",
    "original_PAD = 0\n",
    "original_SOS = 1\n",
    "original_EOS = 2\n",
    "original_UNK = 3\n",
    "\n",
    "w2i = vocab_dict\n",
    "i2w = {k:v for (v,k) in vocab_dict.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for RNN S0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2index(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [vocab_dict[w] for w in tokens]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.path.abspath('')).parent.parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\")\n",
    "print(data_path)\n",
    "corpus = ColorsCorpusReader(os.path.join(data_path,\"colors.csv\"), word_count=None, normalize_colors=True)\n",
    "examples = list(corpus.read())\n",
    "print(\"Number of datapoints: {}\".format(len(examples)))\n",
    "# balance positive and negative samples\n",
    "colors_data = [e.get_context_data()[0] for e in examples]\n",
    "utterance_data = [e.get_context_data()[1] for e in examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepapre test loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_data_tensor = torch.tensor(np.array(colors_data),dtype=torch.float)\n",
    "context_id_data = list(map(sentence2index,utterance_data))\n",
    "max_context_len = np.max([len(c) for c in context_id_data])\n",
    "content_len = [len(c) for c in context_id_data]\n",
    "print(\"MAX length = \",max_context_len)\n",
    "padded_context_data = torch.tensor(np.array([[original_SOS]+c+[original_EOS]+[original_PAD]*(max_context_len-len(c)) for c in context_id_data]))   # <sos>+context+<eos>+<pad>*\n",
    "print(\"Colors shape = \",colors_data_tensor.shape)\n",
    "print(\"Padded context id lists shape = \",padded_context_data.shape)\n",
    "\n",
    "data = [(color,torch.tensor(context,dtype=torch.long),l) for color,context,l in zip(colors_data_tensor,padded_context_data,content_len)]\n",
    "label = torch.zeros(len(data),3)\n",
    "label[:,2] = 1.0\n",
    "print(\"total data length = \",len(data))\n",
    "print(\"total label shape = \",label.shape)\n",
    "\n",
    "test_split = 1000\n",
    "test_data, test_label = data[-test_split:], label[-test_split:]\n",
    "print(\"Test data, Test label length = \",len(test_data),\",\",len(test_label))\n",
    "\n",
    "test_dataset = list(zip(test_data,test_label))\n",
    "test_batch = DataLoader(dataset=test_dataset,batch_size=32,shuffle=False,num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for pre-trained LLM L0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2index(sentence):\n",
    "    tokenized = tokenizer.encode(sentence)\n",
    "    #print(tokenized)\n",
    "    return tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.path.abspath('')).parent.parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\")\n",
    "print(data_path)\n",
    "corpus = ColorsCorpusReader(os.path.join(data_path,\"colors.csv\"), word_count=None, normalize_colors=True)\n",
    "examples = list(corpus.read())\n",
    "print(\"Number of datapoints: {}\".format(len(examples)))\n",
    "# balance positive and negative samples\n",
    "colors_data = [e.get_context_data()[0] for e in examples]\n",
    "utterance_data = [e.get_context_data()[1] for e in examples]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_data_tensor = torch.tensor(np.array(colors_data),dtype=torch.float)\n",
    "context_id_data = list(map(sentence2index,utterance_data))\n",
    "max_context_len = np.max([len(c) for c in context_id_data])\n",
    "print(\"MAX length = \",max_context_len)\n",
    "padded_context_data = torch.tensor(np.array([[SOS]+c+[EOS]+[PAD]*(max_context_len-len(c)) for c in context_id_data]))   # <sos>+context+<eos>+<pad>*\n",
    "print(\"Colors shape = \",colors_data_tensor.shape)\n",
    "print(\"Padded context id lists shape = \",padded_context_data.shape)\n",
    "\n",
    "gpt_data = [(color,torch.tensor(context,dtype=torch.long)) for color,context in zip(colors_data_tensor,padded_context_data)]\n",
    "label = torch.zeros(len(data),3)\n",
    "label[:,2] = 1.0\n",
    "print(\"total data length = \",len(data))\n",
    "print(\"total label shape = \",label.shape)\n",
    "\n",
    "test_split = 1000\n",
    "test_data, test_label = gpt_data[-test_split:], label[-test_split:]\n",
    "print(\"Test data, Test label length = \",len(test_data),\",\",len(test_label))\n",
    "\n",
    "test_dataset = list(zip(test_data,test_label))\n",
    "gpt_test_batch = DataLoader(dataset=test_dataset,batch_size=32,shuffle=False,num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for L0 and L1 accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_labels(lang_probs):\n",
    "    lang_pred = []\n",
    "    for probs in lang_probs:\n",
    "        if probs[0]==probs[1] and probs[1]==probs[2]: # all same\n",
    "            lang_pred.append(int(np.random.randint(3)))\n",
    "        elif probs[0]==probs[1] and max(probs)==probs[0]:\n",
    "            lang_pred.append(int(0 if np.random.randint(2)==0 else 1))\n",
    "        elif probs[1]==probs[2] and max(probs)==probs[1]:\n",
    "            lang_pred.append(int(1 if np.random.randint(2)==0 else 2))\n",
    "        elif probs[0]==probs[2] and max(probs)==probs[1]:\n",
    "            lang_pred.append(int(0 if np.random.randint(2)==0 else 2))\n",
    "        else:\n",
    "            lang_pred.append(int(torch.argmax(probs)))\n",
    "    return np.array(lang_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 comunication accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l0_accuracy(speaker,literal_listener,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    literal_listener.eval()\n",
    "\n",
    "    for i,((cols,lang,x_len),label) in enumerate(test_batch):\n",
    "        cols, lang, x_len, label = cols.to(device), lang.to(device), x_len.to(device), label.to(device)\n",
    "        gen_lang_tensor = speaker.generate(cols, label, tau=5, max_len=max_len)\n",
    "        output_lang = gen_lang_tensor.argmax(2)\n",
    "        lis_labels = literal_listener(cols, output_lang)\n",
    "        pred_labels = get_prob_labels(lis_labels)\n",
    "        correct_labels = np.zeros(cols.shape[0])+2\n",
    "        acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "        accs.append(acc)\n",
    "\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def gpt_lang2L0_lang(generated_langs):\n",
    "    langs = [tokenizer.decode([idx for idx in generated if idx not in [PAD,SOS,EOS]]) for generated in generated_langs]\n",
    "    tokens = [[w2i.get(w,original_UNK) for w in word_tokenize(l)] for l in langs]\n",
    "    max_tokens_len = max([len(t) for t in tokens])\n",
    "    padded_tokens = torch.tensor(np.array([[original_SOS]+ts+[original_EOS]+[original_PAD]*(max_tokens_len-len(ts)) for ts in tokens]))\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_get_l0_accuracy(speaker,literal_listener,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    literal_listener.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,((cols,lang),label) in enumerate(test_batch):\n",
    "            cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "            generated_lang, lang_probs = speaker.generate(tokenizer,cols,label,max_len=max_len)\n",
    "            output_lang = gpt_lang2L0_lang(generated_lang).to(device)\n",
    "            lis_labels = literal_listener(cols, output_lang)\n",
    "            pred_labels = get_prob_labels(lis_labels)\n",
    "            correct_labels = np.zeros(cols.shape[0])+2\n",
    "            acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "            accs.append(acc)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 accuracy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1_accuracy(speaker,test_batch,max_len=5):\n",
    "    accs1 = []\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,((cols,lang,x_len),label) in enumerate(test_batch):\n",
    "            cols, lang, x_len, label = cols.to(device), lang.to(device), x_len.to(device), label.to(device)\n",
    "            # for 1st image\n",
    "            label01 = torch.zeros_like(label)\n",
    "            label01[:0] = 1.0\n",
    "            gen_lang_tensor1 = speaker.generate(cols, label01, tau=5, max_len=max_len)\n",
    "            output_lang1 = gen_lang_tensor1.argmax(2)\n",
    "            # for 2nd image\n",
    "            label02 = torch.zeros_like(label)\n",
    "            label02[:,1] = 1.0\n",
    "            gen_lang_tensor2 = speaker.generate(cols, label02, tau=5, max_len=max_len)\n",
    "            output_lang2 = gen_lang_tensor2.argmax(2)\n",
    "            # for 3rd image\n",
    "            label03 = torch.zeros_like(label)\n",
    "            label03[:,2] = 1.0\n",
    "            gen_lang_tensor3 = speaker.generate(cols, label03, tau=5, max_len=max_len)\n",
    "            output_lang3 = gen_lang_tensor3.argmax(2)\n",
    "            # compute probs\n",
    "            prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                    for batch,(sent,idxs) in enumerate(zip(gen_lang_tensor1,lang))]\n",
    "            prob01_sums = list(map(sum,prob01))\n",
    "            prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                    for batch,(sent,idxs) in enumerate(zip(gen_lang_tensor2,lang))]\n",
    "            prob02_sums = list(map(sum,prob02))\n",
    "            prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                    for batch,(sent,idxs) in enumerate(zip(gen_lang_tensor3,lang))]\n",
    "            prob03_sums = list(map(sum,prob03))\n",
    "            lang_probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "            pred_labels = get_prob_labels(lang_probs)\n",
    "            correct_labels = np.zeros(cols.shape[0])+2\n",
    "            acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "            accs1.append(acc)\n",
    "    return np.mean(accs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_get_l1_accuracy(speaker,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    for i,((cols,lang),label) in enumerate(test_batch):\n",
    "        cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "        # for 1st image\n",
    "        label01 = torch.zeros_like(label)\n",
    "        label01[:,0] = 1.0\n",
    "        generated_lang1, lang_probs1 = speaker.generate(tokenizer,cols,label01,max_len=max_len)\n",
    "        # for 2nd image\n",
    "        label02 = torch.zeros_like(label)\n",
    "        label02[:,1] = 1.0\n",
    "        generated_lang2, lang_probs2 = speaker.generate(tokenizer,cols,label02,max_len=max_len)\n",
    "        # for 3rd image\n",
    "        label03 = torch.zeros_like(label)\n",
    "        label03[:,2] = 1.0\n",
    "        generated_lang3, lang_probs3 = speaker.generate(tokenizer,cols,label03,max_len=max_len)\n",
    "        # compute the probability\n",
    "        prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                for batch,(sent,idxs) in enumerate(zip(lang_probs1,lang))]\n",
    "        prob01_sums = list(map(sum,prob01))\n",
    "        prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                for batch,(sent,idxs) in enumerate(zip(lang_probs2,lang))]\n",
    "        prob02_sums = list(map(sum,prob02))\n",
    "        prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                for batch,(sent,idxs) in enumerate(zip(lang_probs3,lang))]\n",
    "        prob03_sums = list(map(sum,prob03))\n",
    "        probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "        pred_labels = torch.argmax(probs,dim=1)\n",
    "        correct_labels = torch.zeros(cols.shape[0])+2\n",
    "        acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "        accs.append(acc.item())\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare RNN L0 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(y, n):\n",
    "    y_onehot = torch.zeros(y.shape[0], n).to(y.device)\n",
    "    y_onehot.scatter_(1, y.view(-1, 1), 1)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(speaker,literal_listener,criterion,optimizer,train_batch,max_len=5,log=False,do_break=False):\n",
    "    train_loss = 0\n",
    "    #train_l0_acc = 0\n",
    "    #train_l1_acc = 0\n",
    "    for j,((cols,lang,x_len),label) in enumerate(train_batch):\n",
    "        cols, lang, x_len, label = cols.to(device), lang.to(device), x_len.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        speaker.train()\n",
    "        lang_tensor = speaker(cols, label, lang[:,:-1], x_len, tau=1)\n",
    "        output_max_len = lang_tensor.size(1)\n",
    "        lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(vocab_dict.keys())).unsqueeze(0) for sent in lang]))\n",
    "        lang_target = lang_onehot[:,1:output_max_len+1,:]\n",
    "        loss = criterion(lang_tensor.reshape(-1,len(vocab_dict)),lang_target.reshape(-1,len(vocab_dict)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #train_l0_acc += get_l0_accuracy(speaker,literal_listener,train_batch,max_len=max_len)\n",
    "        #train_l1_acc += get_l1_accuracy(speaker,train_batch,max_len=max_len)\n",
    "        if j%100==0 and log: print(j+1,\"/\",len(train_batch))\n",
    "        if do_break: break\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    #batch_train_l0_acc = train_l0_acc/len(train_batch)\n",
    "    #batch_train_l1_acc = train_l1_acc/len(train_batch)\n",
    "    return batch_train_loss#, batch_train_l0_acc, batch_train_l1_acc\n",
    "\n",
    "def eval_model(speaker,literal_listener,criterion,test_batch,max_len=5,log=False,do_break=False):\n",
    "    test_loss = 0\n",
    "    test_l0_acc = 0\n",
    "    test_l1_acc = 0\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for (cols,lang,x_len),label in test_batch:\n",
    "            cols, lang, x_len, label = cols.to(device), lang.to(device), x_len.to(device), label.to(device)\n",
    "            lang_tensor = speaker(cols, label, lang[:,:-1], x_len, tau=1)\n",
    "            output_max_len = lang_tensor.size(1)\n",
    "            lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(vocab_dict.keys())).unsqueeze(0) for sent in lang]))\n",
    "            lang_target = lang_onehot[:,1:output_max_len+1,:]\n",
    "            loss = criterion(lang_tensor.reshape(-1,len(vocab_dict)),lang_target.reshape(-1,len(vocab_dict)))\n",
    "            test_loss += loss.item()\n",
    "            test_l0_acc += get_l0_accuracy(speaker,literal_listener,test_batch,max_len=max_len)\n",
    "            test_l1_acc += get_l1_accuracy(speaker,test_batch,max_len=max_len)\n",
    "            if do_break: break\n",
    "        batch_test_loss = test_loss/len(test_batch)\n",
    "        batch_test_l0_acc = test_l0_acc/len(test_batch)\n",
    "        batch_test_l1_acc = test_l1_acc/len(test_batch)\n",
    "    return batch_test_loss, batch_test_l0_acc, batch_test_l1_acc\n",
    "\n",
    "def train_and_eval_epoch(speaker,literal_listener,criterion,optimizer,epoch,train_batch,test_batch,train_size,max_len=5,log=True,do_break=False):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_l0_list = []\n",
    "    test_l1_list = []\n",
    "    best_loss = 100\n",
    "    best_l0 = 0\n",
    "    best_l1 = 0\n",
    "    for i in range(epoch):\n",
    "        if log:\n",
    "            print(\"##############################################\")\n",
    "            print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "        batch_train_loss = train_model(speaker,literal_listener,criterion,optimizer,train_batch,max_len=max_len,log=log,do_break=do_break)\n",
    "        batch_test_loss, batch_test_l0_acc, batch_test_l1_acc = eval_model(speaker,literal_listener,criterion,test_batch,max_len=max_len,log=log,do_break=do_break)\n",
    "        if log:\n",
    "            print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "            print(\"Test L0 acc:{:.2E}\".format(batch_test_l0_acc))\n",
    "            print(\"Test L1 acc:{:.2E}\".format(batch_test_l1_acc))\n",
    "        train_loss_list.append(batch_train_loss)\n",
    "        test_loss_list.append(batch_test_loss)\n",
    "        test_l0_list.append(batch_test_l0_acc)\n",
    "        test_l1_list.append(batch_test_l1_acc)\n",
    "        if batch_test_loss < best_loss:\n",
    "            if log: print(\"Best loss saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/Baseline/pad-pack-birnn-S0_best-loss_trainSize\"+str(train_size)+\".pth\")\n",
    "            best_loss = batch_test_loss\n",
    "        if batch_test_l0_acc > best_l0:\n",
    "            if log: print(\"Best L0 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/Baseline/pad-pack-birnn-S0_best-l0-acc_trainSize\"+str(train_size)+\".pth\")\n",
    "            best_l0 = batch_test_l0_acc\n",
    "        if batch_test_l1_acc > best_l1:\n",
    "            if log: print(\"Best L0 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/Baseline/pad-pack-birnn-S0_best-l1-acc_trainSize\"+str(train_size)+\".pth\")\n",
    "            best_l1 = batch_test_l1_acc\n",
    "        if do_break: break\n",
    "    return train_loss_list,test_loss_list,test_l0_list,test_l1_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_literal_listener import Emb_RNN_L0\n",
    "from color_literal_speaker import Colors_Feature, RNN_Speaker\n",
    "\n",
    "literal_listener = Emb_RNN_L0(len(vocab_dict)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\emb-rnn-l0_epoch=100_best-acc.pth\",map_location=device))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "emb_dim = 768\n",
    "max_len = 5\n",
    "epoch = 30\n",
    "\n",
    "for train_num in [10,50,250,1250,6250,31250]:\n",
    "    # train_batch\n",
    "    print(\"Train data size = \",train_num)\n",
    "    train_x, train_y = data[:train_num], label[:train_num]\n",
    "    train_dataset = list(zip(train_x,train_y))\n",
    "    train_batch = DataLoader(dataset=train_dataset,batch_size=32,shuffle=True,num_workers=0)\n",
    "    # model setting\n",
    "    speaker_embs = nn.Embedding(len(vocab_dict), emb_dim)\n",
    "    speaker_feat = Colors_Feature(output_size=16)\n",
    "    speaker = RNN_Speaker(speaker_feat, speaker_embs).to(device)\n",
    "    optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "    # train and eval with epoch\n",
    "    tr_loss,ts_loss,ts_l0,ts_l1 = train_and_eval_epoch(speaker,literal_listener,\\\n",
    "        criterion,optimizer,epoch,train_batch,test_batch,train_size=train_num,max_len=max_len,log=False,do_break=True)\n",
    "    metrics = np.array([tr_loss,ts_loss,ts_l0,ts_l1])\n",
    "    np.save(\"metrics/Baseline/baseline-s0_trainSize=\"+str(train_num)+\".npy\",metrics)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPT-2 S0 model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(speaker,literal_listener,criterion,optimizer,train_batch,max_len=5,log=False,do_break=False):\n",
    "    train_loss = 0\n",
    "    #train_l0_acc = 0\n",
    "    #train_l1_acc = 0\n",
    "    speaker.train()\n",
    "    for j,((cols,lang),label) in enumerate(train_batch):\n",
    "        cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = speaker(cols, label, lang)\n",
    "        output_view = output.view(-1, output.shape[-1])\n",
    "        target = lang[:,1:].reshape(-1)\n",
    "        loss = criterion(output_view, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        #train_l0_acc += gpt_get_l0_accuracy(speaker,literal_listener,train_batch,max_len=max_len)\n",
    "        #train_l1_acc += gpt_get_l1_accuracy(speaker,train_batch,max_len=max_len)\n",
    "        if j%100==0 and log: print(j+1,\"/\",len(train_batch))\n",
    "        if do_break: break\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    #batch_train_l0_acc = train_l0_acc/len(train_batch)\n",
    "    #batch_train_l1_acc = train_l1_acc/len(train_batch)\n",
    "    return batch_train_loss#, batch_train_l0_acc, batch_train_l1_acc\n",
    "\n",
    "def eval_model(speaker,literal_listener,criterion,test_batch,max_len=5,log=False,do_break=False):\n",
    "    test_loss = 0\n",
    "    test_l0_acc = 0\n",
    "    test_l1_acc = 0\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for (cols,lang),label in test_batch:\n",
    "            cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "            output = speaker(cols, label, lang)\n",
    "            output_view = output.view(-1, output.shape[-1])\n",
    "            target = lang[:,1:].reshape(-1)\n",
    "            loss = criterion(output_view, target)\n",
    "            test_loss += loss.item()\n",
    "            test_l0_acc += gpt_get_l0_accuracy(speaker,literal_listener,test_batch,max_len=max_len)\n",
    "            test_l1_acc += gpt_get_l1_accuracy(speaker,test_batch,max_len=max_len)\n",
    "            if do_break: break\n",
    "        batch_test_loss = test_loss/len(test_batch)\n",
    "        batch_test_l0_acc = test_l0_acc/len(test_batch)\n",
    "        batch_test_l1_acc = test_l1_acc/len(test_batch)\n",
    "    return batch_test_loss, batch_test_l0_acc, batch_test_l1_acc\n",
    "\n",
    "def train_and_eval_epoch(speaker,literal_listener,criterion,optimizer,epoch,train_batch,test_batch,train_size,max_len=5,log=True,do_break=False):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    train_l0_list = []\n",
    "    test_l0_list = []\n",
    "    train_l1_list = []\n",
    "    test_l1_list = []\n",
    "    best_loss = 100\n",
    "    best_l0 = 0\n",
    "    best_l1 = 0\n",
    "    for i in range(epoch):\n",
    "        if log:\n",
    "            print(\"##############################################\")\n",
    "            print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "        #batch_train_loss, batch_train_l0_acc, batch_train_l1_acc = train_model(speaker,literal_listener,criterion,optimizer,train_batch,max_len=max_len,log=log,do_break=do_break)\n",
    "        batch_train_loss = train_model(speaker,literal_listener,criterion,optimizer,train_batch,max_len=max_len,log=log,do_break=do_break)\n",
    "        batch_test_loss, batch_test_l0_acc, batch_test_l1_acc = eval_model(speaker,literal_listener,criterion,test_batch,max_len=max_len,log=log,do_break=do_break)\n",
    "        if log:\n",
    "            print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "            #print(\"Train L0 acc:{:.2E}, Test L0 acc:{:.2E}\".format(batch_train_l0_acc,batch_test_l0_acc))\n",
    "            #print(\"Train L1 acc:{:.2E}, Test L1 acc:{:.2E}\".format(batch_train_l1_acc,batch_test_l1_acc))\n",
    "            print(\"Test L0 acc:{:.2E}\".format(batch_test_l0_acc))\n",
    "            print(\"Test L1 acc:{:.2E}\".format(batch_test_l1_acc))\n",
    "        train_loss_list.append(batch_train_loss)\n",
    "        test_loss_list.append(batch_test_loss)\n",
    "        #train_l0_list.append(batch_train_l0_acc)\n",
    "        test_l0_list.append(batch_test_l0_acc)\n",
    "        #train_l1_list.append(batch_train_l1_acc)\n",
    "        test_l1_list.append(batch_test_l1_acc)\n",
    "        if batch_test_loss < best_loss:\n",
    "            if log: print(\"Best loss saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/GPT2/gpt-2-S0_best-loss_trainSize\"+str(train_size)+\".pth\")\n",
    "            best_loss = batch_test_loss\n",
    "        if batch_test_l0_acc > best_l0:\n",
    "            if log: print(\"Best L0 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/GPT2/gpt-2-S0_best-l0-acc_trainSize\"+str(train_size)+\".pth\")\n",
    "            best_l0 = batch_test_l0_acc\n",
    "        if batch_test_l1_acc > best_l1:\n",
    "            if log: print(\"Best L0 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/GPT2/gpt-2-S0_best-l1-acc_trainSize\"+str(train_size)+\".pth\")\n",
    "            best_l1 = batch_test_l1_acc\n",
    "        if do_break: break\n",
    "    return train_loss_list,test_loss_list,test_l0_list,test_l1_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from color_literal_listener import Emb_RNN_L0\n",
    "from color_literal_speaker import S0_EncoderDecoder\n",
    "\n",
    "literal_listener = Emb_RNN_L0(len(vocab_dict)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\emb-rnn-l0_epoch=100_best-acc.pth\",map_location=device))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "max_len = 5\n",
    "epoch = 30\n",
    "\n",
    "for train_num in [10,50,250,1250,6250,31250]:\n",
    "    # train_batch\n",
    "    print(\"Train data size = \",train_num)\n",
    "    train_x, train_y = gpt_data[:train_num], label[:train_num]\n",
    "    train_dataset = list(zip(train_x,train_y))\n",
    "    train_batch = DataLoader(dataset=train_dataset,batch_size=16,shuffle=True,num_workers=0)\n",
    "    # model setting\n",
    "    speaker = S0_EncoderDecoder(input_size=3).to(device)\n",
    "    optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "    # train and eval with epoch\n",
    "    tr_loss,ts_loss,ts_l0,ts_l1 = train_and_eval_epoch(speaker,literal_listener,\\\n",
    "        criterion,optimizer,epoch,train_batch,gpt_test_batch,train_size=train_num,max_len=max_len,log=False,do_break=True)\n",
    "    metrics = np.array([tr_loss,ts_loss,ts_l0,ts_l1])\n",
    "    np.save(\"metrics/GPT2/gpt2-s0_trainSize=\"+str(train_num)+\".npy\",metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
