{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from shapeworld_data import load_raw_data, get_vocab, ShapeWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device = \",device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    tokenized = tokenizer.encode(sentence)\n",
    "    #print(tokenized)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.path.abspath('')).parent.parent.parent.absolute()\n",
    "data_path = os.path.join(root,\"data\\shapeworld_np\")\n",
    "print(data_path)\n",
    "data_list = os.listdir(data_path)\n",
    "print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab([os.path.join(data_path,d) for d in data_list])\n",
    "print(vocab[\"w2i\"])\n",
    "\n",
    "COLOR = {\"white\":[1,0,0,0,0,0], \"green\":[0,1,0,0,0,0], \"gray\":[0,0,1,0,0,0], \"yellow\":[0,0,0,1,0,0], \"red\":[0,0,0,0,1,0], \"blue\":[0,0,0,0,0,1], \"other\":[0,0,0,0,0,0]}\n",
    "SHAPE = {\"shape\":[0,0,0,0], \"square\":[1,0,0,0], \"circle\":[0,1,0,0], \"rectangle\":[0,0,1,0], \"ellipse\":[0,0,0,1]}\n",
    "\n",
    "print(\"Generating Vocab_dict from GPT tokenizer ...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "gpt_vocab_dict = tokenizer.get_vocab()\n",
    "print(\"Length of the GPT Vocab list is \",len(gpt_vocab_dict.keys()))\n",
    "\n",
    "PAD = 15636\n",
    "SOS= EOS = UNK = 50256\n",
    "original_PAD = 0\n",
    "original_SOS = 1\n",
    "original_EOS = 2\n",
    "original_UNK = 3\n",
    "\n",
    "w2i = vocab[\"w2i\"]\n",
    "i2w = vocab[\"i2w\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for RNN S0 model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepapre test loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[0]))\n",
    "imgs = d[\"imgs\"]\n",
    "labels = d[\"labels\"]\n",
    "langs = d[\"langs\"]\n",
    "for i in range(1,4):\n",
    "    d = load_raw_data(os.path.join(data_path,data_list[i]))\n",
    "    imgs = np.vstack((imgs,d[\"imgs\"]))\n",
    "    labels = np.vstack((labels,d[\"labels\"]))\n",
    "    langs = np.hstack((langs,d[\"langs\"]))\n",
    "d[\"imgs\"] = imgs\n",
    "d[\"labels\"] = labels\n",
    "d[\"langs\"] = langs\n",
    "print(d[\"imgs\"].shape, d[\"labels\"].shape, d[\"langs\"].shape)\n",
    "data = [(img,label,lang) for img,label,lang in ShapeWorld(d, vocab)]\n",
    "print(len(data))\n",
    "print(data[0][0].shape, data[0][1].shape, data[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[-1]))\n",
    "print(d[\"imgs\"].shape)\n",
    "print(d[\"labels\"].shape)\n",
    "print(d[\"langs\"].shape)\n",
    "test_batch = DataLoader(ShapeWorld(d, vocab), batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for pre-trained LLM S0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2index(sentence):\n",
    "    tokenized = tokenizer.encode(sentence)\n",
    "    #print(tokenized)\n",
    "    return tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_raw_data(os.path.join(data_path,data_list[0]))\n",
    "imgs = d[\"imgs\"]\n",
    "labels = d[\"labels\"]\n",
    "langs = d[\"langs\"]\n",
    "for i in range(1,5):\n",
    "    d = load_raw_data(os.path.join(data_path,data_list[i]))\n",
    "    imgs = np.vstack((imgs,d[\"imgs\"]))\n",
    "    labels = np.vstack((labels,d[\"labels\"]))\n",
    "    langs = np.hstack((langs,d[\"langs\"]))\n",
    "\n",
    "imgs_data_tensor = torch.tensor(imgs,dtype=torch.float)\n",
    "label_data_tensor = torch.tensor(labels)\n",
    "context_id_data = list(map(sentence2index,langs))\n",
    "max_context_len = np.max([len(c) for c in context_id_data])\n",
    "padded_context_data = torch.tensor(np.array([[SOS]+c+[EOS]+[PAD]*(max_context_len-len(c)) for c in context_id_data]))   # <sos>+context+<eos>+<pad>*\n",
    "print(imgs_data_tensor.shape, label_data_tensor.shape, padded_context_data.shape)\n",
    "\n",
    "gpt_data = [(img,u,l) for img,l,u in zip(imgs_data_tensor,label_data_tensor,padded_context_data)]\n",
    "test_split = 1000\n",
    "gpt_train_data, gpt_test_data = gpt_data[:-test_split], gpt_data[-test_split:]\n",
    "print(\"Train, Test data length = \",len(gpt_train_data),\",\",len(gpt_test_data))\n",
    "\n",
    "gpt_test_batch = DataLoader(dataset=gpt_test_data,batch_size=32,shuffle=False,num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for L0 and L1 accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_labels(lang_probs):\n",
    "    lang_pred = []\n",
    "    for probs in lang_probs:\n",
    "        if probs[0]==probs[1] and probs[1]==probs[2]: # all same\n",
    "            lang_pred.append(int(np.random.randint(3)))\n",
    "        elif probs[0]==probs[1] and max(probs)==probs[0]:\n",
    "            lang_pred.append(int(0 if np.random.randint(2)==0 else 1))\n",
    "        elif probs[1]==probs[2] and max(probs)==probs[1]:\n",
    "            lang_pred.append(int(1 if np.random.randint(2)==0 else 2))\n",
    "        elif probs[0]==probs[2] and max(probs)==probs[1]:\n",
    "            lang_pred.append(int(0 if np.random.randint(2)==0 else 2))\n",
    "        else:\n",
    "            lang_pred.append(int(torch.argmax(probs)))\n",
    "    return np.array(lang_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 comunication accuracy computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l0_accuracy(speaker,literal_listener,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    literal_listener.eval()\n",
    "    speaker.eval()\n",
    "    for i,(cols,label,lang) in enumerate(test_batch):\n",
    "        cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "        lang_tensor = speaker.generate(cols, label, max_len=max_len)\n",
    "        output_lang = lang_tensor.argmax(2)\n",
    "        lis_labels = literal_listener(cols, output_lang)\n",
    "        pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "        correct_labels = torch.zeros(cols.shape[0])\n",
    "        acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "        accs.append(acc.item())\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = list(COLOR.keys())\n",
    "col_list[-1] = \"\"\n",
    "shape_list = list(SHAPE.keys())\n",
    "utter_list = [\" \".join([w for w in (c+\" \"+s).split(\" \") if w]) for c in col_list for s in shape_list+[\"\"]]\n",
    "gpt_utter_list = [\"\".join([w for w in (c+\" \"+s).split(\" \") if w]) for c in col_list for s in shape_list+[\"\"]]\n",
    "vocab2gpt = {g:u for u,g in zip(utter_list,gpt_utter_list)}\n",
    "for g,u in vocab2gpt.items():\n",
    "    print(u,\" : \",g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "\n",
    "def decode_gpt_vocab(w):\n",
    "    if w in w2i.keys(): \n",
    "        return [w2i[w]]\n",
    "    elif w in vocab2gpt.keys():\n",
    "        return [w2i[t] for t in vocab2gpt[w].split(\" \")]\n",
    "    else:\n",
    "        return [original_UNK]\n",
    "\n",
    "def gpt_lang2L0_lang(generated_langs):\n",
    "    langs = [tokenizer.decode([idx for idx in generated if idx not in [PAD,SOS,EOS]]) for generated in generated_langs]\n",
    "    tokens = []\n",
    "    for l in langs:\n",
    "        decoded = [decode_gpt_vocab(w) for w in word_tokenize(l)]+[[],[]]\n",
    "        tokens.append(list(reduce(lambda x,y:x+y,decoded)))\n",
    "    max_tokens_len = max([len(t) for t in tokens])\n",
    "    padded_tokens = torch.tensor(np.array([[original_SOS]+ts+[original_EOS]+[original_PAD]*(max_tokens_len-len(ts)) for ts in tokens]))\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_get_l0_accuracy(speaker,literal_listener,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,(cols,lang,label) in enumerate(test_batch):\n",
    "            cols, lang, label = cols.to(device), lang.to(device), label.to(device)\n",
    "            generated_lang, lang_probs = speaker.generate(tokenizer,cols,label,max_len=max_len)\n",
    "            output_lang = gpt_lang2L0_lang(generated_lang).to(device)\n",
    "            literal_listener.eval()\n",
    "            lis_labels = literal_listener(cols, output_lang)\n",
    "            pred_labels = torch.argmax(lis_labels,dim=1)\n",
    "            correct_labels = torch.zeros(cols.shape[0])\n",
    "            acc = sum(correct_labels.to(device)==pred_labels)/len(correct_labels)\n",
    "            accs.append(acc.item())\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 accuracy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1_accuracy(speaker,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,(cols,label,lang) in enumerate(test_batch):\n",
    "            cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "            # for 1st image\n",
    "            label01 = torch.zeros_like(label)\n",
    "            label01[:,0] = 1.0\n",
    "            lang_tensor1 = speaker.generate(cols, label01, max_len=max_len)\n",
    "            # for 2nd image\n",
    "            label02 = torch.zeros_like(label)\n",
    "            label02[:,1] = 1.0\n",
    "            lang_tensor2 = speaker.generate(cols, label02, max_len=max_len)\n",
    "            # for 3rd image\n",
    "            label03 = torch.zeros_like(label)\n",
    "            label03[:,2] = 1.0\n",
    "            lang_tensor3 = speaker.generate(cols, label03, max_len=max_len)\n",
    "            # compute probs\n",
    "            prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                        for batch,(sent,idxs) in enumerate(zip(lang_tensor1,lang))]\n",
    "            prob01_sums = list(map(sum,prob01))\n",
    "            prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                        for batch,(sent,idxs) in enumerate(zip(lang_tensor2,lang))]\n",
    "            prob02_sums = list(map(sum,prob02))\n",
    "            prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                        for batch,(sent,idxs) in enumerate(zip(lang_tensor3,lang))]\n",
    "            prob03_sums = list(map(sum,prob03))\n",
    "            probs = torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1)\n",
    "            pred_labels = get_prob_labels(probs)\n",
    "            correct_labels = np.zeros(cols.shape[0])\n",
    "            acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "            accs.append(acc.item())\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_get_l1_accuracy(speaker,test_batch,max_len=5):\n",
    "    accs = []\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,(cols,lang,label) in enumerate(test_batch):\n",
    "            cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "            # for 1st image\n",
    "            label01 = torch.zeros_like(label)\n",
    "            label01[:,0] = 1.0\n",
    "            generated_lang1, lang_probs1 = speaker.generate(tokenizer,cols,label01,max_len=max_len)\n",
    "            # for 2nd image\n",
    "            label02 = torch.zeros_like(label)\n",
    "            label02[:,1] = 1.0\n",
    "            generated_lang2, lang_probs2 = speaker.generate(tokenizer,cols,label02,max_len=max_len)\n",
    "            # for 3rd image\n",
    "            label03 = torch.zeros_like(label)\n",
    "            label03[:,2] = 1.0\n",
    "            generated_lang3, lang_probs3 = speaker.generate(tokenizer,cols,label03,max_len=max_len)\n",
    "            # compute the probability\n",
    "            prob01 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                for batch,(sent,idxs) in enumerate(zip(lang_probs1,lang))]\n",
    "            prob01_sums = list(map(sum,prob01))\n",
    "            prob02 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                for batch,(sent,idxs) in enumerate(zip(lang_probs2,lang))]\n",
    "            prob02_sums = list(map(sum,prob02))\n",
    "            prob03 = [[torch.log(word_dist[idx]+0.001).to(\"cpu\").detach() for word_dist,idx in zip(sent,idxs)] \\\n",
    "                for batch,(sent,idxs) in enumerate(zip(lang_probs3,lang))]\n",
    "            prob03_sums = list(map(sum,prob03))\n",
    "            probs = F.softmax(torch.tensor(np.array([prob01_sums,prob02_sums,prob03_sums])).transpose(0,1),dim=-1)\n",
    "            pred_labels = torch.argmax(probs,dim=1)\n",
    "            correct_labels = torch.zeros(cols.shape[0])\n",
    "            acc = sum(correct_labels==pred_labels)/len(correct_labels)\n",
    "            accs.append(acc.item())\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare RNN L0 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(y, n):\n",
    "    y_onehot = torch.zeros(y.shape[0], n).to(y.device)\n",
    "    y_onehot.scatter_(1, y.view(-1, 1), 1)\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(speaker,criterion,optimizer,train_batch,log=False,do_break=False):\n",
    "    train_loss = 0\n",
    "    speaker.train()\n",
    "    for cols,label,lang in train_batch:\n",
    "        cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        x_lens = torch.tensor(np.array([3]*len(cols))).to(device)\n",
    "        lang_tensor = speaker(cols, label, lang[:,:-1], x_lens=x_lens)\n",
    "        output_max_len = lang_tensor.size(1)\n",
    "        lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(w2i)).unsqueeze(0) for sent in lang]))\n",
    "        lang_target = lang_onehot[:,1:output_max_len+1,:]\n",
    "        loss = criterion(lang_tensor.reshape(-1, len(w2i)), lang_target.reshape(-1,len(w2i)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if do_break: break\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    return batch_train_loss\n",
    "\n",
    "def eval_model(speaker,literal_listener,criterion,test_batch,max_len=5,log=False,do_break=False):\n",
    "    test_loss = 0\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for cols,label,lang in test_batch:\n",
    "            cols, lang, label = cols.to(device).to(torch.float), lang.to(device), label.to(device).to(torch.float)\n",
    "            x_lens = torch.tensor(np.array([3]*len(cols))).to(device)\n",
    "            lang_tensor = speaker(cols, label, lang[:,:-1], x_lens=x_lens)\n",
    "            output_max_len = lang_tensor.size(1)\n",
    "            lang_onehot = torch.vstack(tuple([to_onehot(sent.to(torch.int64) ,len(w2i)).unsqueeze(0) for sent in lang]))\n",
    "            lang_target = lang_onehot[:,1:output_max_len+1,:]\n",
    "            loss = criterion(lang_tensor.reshape(-1, len(w2i)), lang_target.reshape(-1, len(w2i)))\n",
    "            test_loss += loss.item()\n",
    "            if do_break: break\n",
    "    batch_test_loss = test_loss/len(test_batch)\n",
    "    batch_test_l0_acc = get_l0_accuracy(speaker,literal_listener,test_batch,max_len=max_len)\n",
    "    batch_test_l1_acc = get_l1_accuracy(speaker,test_batch,max_len=max_len)\n",
    "    return batch_test_loss, batch_test_l0_acc, batch_test_l1_acc\n",
    "\n",
    "def train_and_eval_epochs(speaker,literal_listener,criterion,optimizer,epoch,train_batch,test_batch,train_size,max_len=5,log=False,do_break=False):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_l0_acc_list = []\n",
    "    test_l1_acc_list = []\n",
    "    best_loss = 100\n",
    "    best_l0_acc = 0\n",
    "    best_l1_acc = 0\n",
    "    for i in range(epoch):\n",
    "        if log:\n",
    "            print(\"##############################################\")\n",
    "            print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "        literal_listener.train()\n",
    "        batch_train_loss = train_model(speaker,criterion,optimizer,train_batch,log=log,do_break=do_break)\n",
    "        batch_test_loss,batch_test_l0_acc,batch_test_l1_acc = eval_model(speaker,literal_listener,criterion,test_batch,max_len=max_len,log=log,do_break=do_break)\n",
    "        if log:\n",
    "            print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "            print(\"Test L0 Acc:{:.2E}, Test L1 Acc:{:.2E}\".format(batch_test_l0_acc,batch_test_l1_acc))\n",
    "        train_loss_list.append(batch_train_loss)\n",
    "        test_loss_list.append(batch_test_loss)\n",
    "        test_l0_acc_list.append(batch_test_l0_acc)\n",
    "        test_l1_acc_list.append(batch_test_l1_acc)\n",
    "        if batch_test_loss < best_loss:\n",
    "            if log: print(\"Best loss saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/Baseline/shapeworld_RNN-S0_best-loss_trainSize=\"+str(train_size)+\".pth\")\n",
    "            best_loss = batch_test_loss\n",
    "        if batch_test_l0_acc > best_l0_acc:\n",
    "            if log: print(\"Best L0 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/Baseline/shapeworld_RNN-S0_best-l0-acc_trainSize=\"+str(train_size)+\".pth\")\n",
    "            best_l0_acc = batch_test_l0_acc\n",
    "        if batch_test_l1_acc > best_l1_acc:\n",
    "            if log: print(\"Best L1 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/Baseline/shapeworld_RNN-S0_best-l1-acc_trainSize=\"+str(train_size)+\".pth\")\n",
    "            best_l1_acc = batch_test_l1_acc\n",
    "        if do_break: break\n",
    "    return train_loss_list, test_loss_list, test_l0_acc_list, test_l1_acc_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0\n",
    "from literal_speaker_shapeworld import CS_CNN_Encoder, RNN_Speaker\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "emb_dim = 768\n",
    "feat_dim = 100\n",
    "max_len = 5\n",
    "epoch = 10\n",
    "\n",
    "for train_num in [15,60,250,1000,4000]:\n",
    "    # train_batch\n",
    "    print(\"Train data size = \",train_num)\n",
    "    train_batch = DataLoader(dataset=data[:train_num],batch_size=32,shuffle=True,num_workers=0)\n",
    "    # model setting\n",
    "    speaker_embs = nn.Embedding(len(w2i), emb_dim)\n",
    "    speaker_feat = CS_CNN_Encoder(output_size=feat_dim,device=device)\n",
    "    speaker = RNN_Speaker(speaker_feat, speaker_embs, feat_size=feat_dim).to(device)\n",
    "    optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "    # train and eval with epoch\n",
    "    tr_loss,ts_loss,ts_l0,ts_l1 = train_and_eval_epochs(speaker,literal_listener,\\\n",
    "        criterion,optimizer,epoch,train_batch,test_batch,train_size=train_num,max_len=max_len,log=False,do_break=True)\n",
    "    metrics = np.array([tr_loss,ts_loss,ts_l0,ts_l1])\n",
    "    np.save(\"metrics/Baseline/baseline-s0_trainSize=\"+str(train_num)+\".npy\",metrics)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GPT-2 S0 model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(speaker,criterion,optimizer,train_batch,do_break=False):\n",
    "    train_loss= 0\n",
    "    speaker.train()\n",
    "    for cols,lang,label in train_batch:\n",
    "        cols, lang, label = cols.to(device), lang.type(torch.LongTensor).to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = speaker(cols, label, lang)\n",
    "        output_view = output.view(-1, output.shape[-1])\n",
    "        target = lang[:,1:].reshape(-1)\n",
    "        lang_loss = criterion(output_view, target)\n",
    "        lang_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += lang_loss.item()\n",
    "        if do_break: break\n",
    "    batch_train_loss = train_loss/len(train_batch)\n",
    "    return batch_train_loss\n",
    "\n",
    "def eval_model(speaker,literal_listener,criterion,test_batch,max_len=5,do_break=False):\n",
    "    test_loss = 0\n",
    "    speaker.eval()\n",
    "    with torch.no_grad():\n",
    "        for cols,lang,label in test_batch:\n",
    "            cols, lang, label = cols.to(device), lang.type(torch.LongTensor).to(device), label.to(device)\n",
    "            output = speaker(cols, label, lang)\n",
    "            output_view = output.view(-1, output.shape[-1])\n",
    "            target = lang[:,1:].reshape(-1)\n",
    "            lang_loss = criterion(output_view, target)\n",
    "            test_loss += lang_loss.item()\n",
    "            if do_break: break\n",
    "        batch_test_loss = test_loss/len(test_batch)\n",
    "        batch_test_l0_acc = gpt_get_l0_accuracy(speaker,literal_listener,test_batch,max_len=max_len)\n",
    "        batch_test_l1_acc = gpt_get_l1_accuracy(speaker,test_batch,max_len=max_len)\n",
    "    return batch_test_loss, batch_test_l0_acc, batch_test_l1_acc\n",
    "\n",
    "def train_and_eval_epochs(speaker,literal_listener,criterion,optimizer,epoch,train_batch,test_batch,train_size,max_len=5,log=False,do_break=False):\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_l0_acc_list = []\n",
    "    test_l1_acc_list = []\n",
    "    best_loss = 100\n",
    "    best_l0_acc = 0\n",
    "    best_l1_acc = 0\n",
    "    for i in range(epoch):\n",
    "        if log:\n",
    "            print(\"##############################################\")\n",
    "            print(\"Epoch:{}/{}\".format(i+1,epoch))\n",
    "        batch_train_loss = train_model(speaker,criterion,optimizer,train_batch,do_break=do_break)\n",
    "        batch_test_loss,batch_test_l0_acc,batch_test_l1_acc = eval_model(speaker,literal_listener,criterion,test_batch,max_len=max_len,do_break=do_break) \n",
    "        if log:\n",
    "            print(\"Train Loss:{:.2E}, Test Loss:{:.2E}\".format(batch_train_loss,batch_test_loss))\n",
    "            print(\"Test L0 Acc:{:.2E}, Test L1 Acc:{:.2E}\".format(batch_test_l0_acc,batch_test_l1_acc))\n",
    "        train_loss_list.append(batch_train_loss)\n",
    "        test_loss_list.append(batch_test_loss)\n",
    "        test_l0_acc_list.append(batch_test_l0_acc)\n",
    "        test_l1_acc_list.append(batch_test_l1_acc)\n",
    "        if batch_test_loss < best_loss:\n",
    "            if log: print(\"Best loss saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/GPT2/shapeworld_gpt2-S0_best-loss_trainSize=\"+str(train_size)+\".pth\")\n",
    "            best_loss = batch_test_loss\n",
    "        if batch_test_l0_acc > best_l0_acc:\n",
    "            if log: print(\"Best L0 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/GPT2/shapeworld_gpt2-S0_best-l0-acc_trainSize=\"+str(train_size)+\".pth\")\n",
    "            best_l0_acc = batch_test_l0_acc\n",
    "        if batch_test_l1_acc > best_l1_acc:\n",
    "            if log: print(\"Best L1 acc saved ...\")\n",
    "            torch.save(speaker.to(device).state_dict(),\"model_params/GPT2/shapeworld_gpt2-S0_best-l1-acc_trainSize=\"+str(train_size)+\".pth\")\n",
    "            best_l1_acc = batch_test_l1_acc\n",
    "        if do_break: break\n",
    "    return train_loss_list, test_loss_list, test_l0_acc_list, test_l1_acc_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from literal_listener_shapeworld import ShapeWorld_RNN_L0\n",
    "from literal_speaker_shapeworld import S0_EncoderDecoder\n",
    "\n",
    "literal_listener = ShapeWorld_RNN_L0(len(w2i)).to(device)\n",
    "literal_listener.load_state_dict(torch.load(\"model_params\\shapeworld_rnn_full-data_100epoch_l0_last.pth\",map_location=device))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "emb_dim = 768\n",
    "feat_dim = 10\n",
    "max_len = 5\n",
    "epoch = 10\n",
    "\n",
    "for train_num in [15,60,250,1000,4000]:\n",
    "    # train_batch\n",
    "    print(\"Train data size = \",train_num)\n",
    "    train_batch = DataLoader(dataset=gpt_data[:train_num],batch_size=16,shuffle=True,num_workers=0)\n",
    "    # model setting\n",
    "    speaker = S0_EncoderDecoder(input_size=feat_dim).to(device)\n",
    "    optimizer = optim.Adam(list(speaker.parameters()),lr=0.001)\n",
    "    # train and eval with epoch\n",
    "    tr_loss,ts_loss,ts_l0,ts_l1 = train_and_eval_epochs(speaker,literal_listener,\\\n",
    "        criterion,optimizer,epoch,train_batch,gpt_test_batch,train_size=train_num,max_len=max_len,log=False,do_break=True)\n",
    "    metrics = np.array([tr_loss,ts_loss,ts_l0,ts_l1])\n",
    "    np.save(\"metrics/GPT2/gpt2-s0_trainSize=\"+str(train_num)+\".npy\",metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
